#+TITLE: Multisensor Validation Log
#+DATE: <2016-06-30>
#+AUTHOR: Vladimir Alexiev
#+EMAIL: vladimir.alexiev@ontotext.com
#+OPTIONS: ':nil *:t -:t ::t <:t H:5 \n:nil ^:{} arch:headline author:t c:nil
#+OPTIONS: creator:comment d:(not "LOGBOOK") date:t e:t email:nil f:t inline:t num:t
#+OPTIONS: p:nil pri:nil stat:t tags:t tasks:t tex:t timestamp:nil toc:3 todo:t |:t
#+CREATOR: Emacs 25.0.50.1 (Org mode 8.2.10)
#+DESCRIPTION:
#+EXCLUDE_TAGS: noexport
#+KEYWORDS:
#+LANGUAGE: en
#+SELECT_TAGS: export
#+TODO: TODO QUE | DONE CANCEL

[[https://github.com/VladimirAlexiev/VladimirAlexiev.github.io/blob/master/Multisensor/validation.org][Github Source]], [[http://VladimirAlexiev.github.io/Multisensor/validation.html][HTML Rendered version]]

* Intro
From now until the end of the project I'll keep a detailed log of what I validated and defects I found.
I'll use Org-mode mechanisms to track the defects (tags TODO, DONE, CANCELED) and try to monitor email conversation to keep this up to date.
The numbers in brackets after a section name show the resolved vs total defects.
Whenever a defect is posted in Jira, I'll track the issue number.

Please contact me on skype://valexiev1 for corrections/additions.
Even better, you could [[https://github.com/VladimirAlexiev/VladimirAlexiev.github.io/edit/master/Multisensor/validation.org][edit this file on github]] (it's a plain text file!) and send me a pull request.
- Mistakes or imprecise info (eg the scope of a particular service is described wrong, or a particular issue is in fact a non-issue)
- When an issue is resolved
- When an issue should be canceled
I'll also attend weekly calls related to RDF data and validation, and update this.

IMPORTANT: Just because an issue is listed in the section for a particular service, doesn't necessarily mean that service created the defect!

** Queries
Below I also track questions related to queries.
But when agreed, they should be moved to gdoc [[https://docs.google.com/document/d/1FfkiiTYvrLzHJ5P5j34NRVGPbXml0ndpNtiNbH2osRw/edit][Multisensor SPARQL Queries]].

Or we could simply use the doc:
if you need a new query, or a query needs optimization, please write a comment in red,
and notify me in a gdoc comment (add +vladimir@sirma.bg to the comment).

* Validation [1/16]

** Entity Linking Service [0/1]

*** TODO Underscores to Spaces
The EL service emits Babelnet entity labels in up to 4 languages, eg
#+BEGIN_SRC Turtle
bn:s00088614v  skos:prefLabel  "zu_befriedigen"@de , "satisfacer"@es , "satisfaire"@fr , "задоволи"@bg .
bn:s00014609n  skos:prefLabel  "Kuchen"@de , "Pastel_(gastronomía)"@es , "Gâteau"@fr , "Торта"@bg .
bn:s01718102n  skos:prefLabel  "I_do_not_want_what_I_haven't_got"@es , "I_Don't_Want_What_I_Haven't_Got"@en , "I_Do_Not_Want_What_I_Haven't_Got"@fr .
bn:s02229586n  skos:prefLabel  "UHC_Hamburg"@en , "Uhlenhorster_HC"@fr .
#+END_SRC
For reasons unknown, Babelnet uses underscores (eg see [[http://babelnet.org/rdf/page/UHC_Hamburg_n_EN][UHC_Hamburg_n_EN]]).
I think EL should convert the underscores to spaces to make the label more natural.

Should we also remove disambiguations, which are trailing parenthesized parts, eg "Pastel_(gastronomía)" -> "Pastel"?
Since these labels are not used for NLP tasks, and the disambiguations are very useful for understanding what the entity is, let's leave them.

** Advanced Context Extraction Service [1/3]
<2016-06-23 Thu>: checked [[http://grinder1.multisensorproject.eu/cepfiles/rdf_validation/context_extraction_validation/0e6b24-CONTEXT_EXTRACTION-22-6-2016.ttl][0e6b24-CONTEXT_EXTRACTION-22-6-2016.ttl]]

This service adds new Text Characteristics properties (technicality, fluency, richness) to the context.

*** DONE Wrong prefix for Text Characteristics
Text Characteristics (technicality, fluency, richness):
- currently use <http://data.multisensor.eu/ontology#>
- but the correct prefix is <http://data.multisensorproject.eu/ontology#>

Victor <2016-06-30 Thu>: updated the prefix

*** TODO Crawler to decode HTML entities
It would be good if the crawler decodes HTML entities before storing in dc:subject (possibly also dc:title, dc:description). Eg:
#+BEGIN_SRC Turtle
dc:subject
  "Europ&auml;ische Union, ISIN_FR0003500008, Brexit, Erholungskurs, Europa, Paris, Gro&szlig;britannien" ,
  "Economy, Business & Finance" ;
#+END_SRC

*** TODO Keywords vs Category
Victor: both of the following fields extracted by the crawler are mapped to dc:subject:
- category: eg "Economy, Business & Finance" 
- keywords: eg "Europaische Union, ISIN_FR0003500008, Brexit, Erholungskurs, Europa, Paris, Grossbritannien"
Should we separe them in different properties?

Vladimir: 
[[https://iptc.org][IPTC]] is the [[https://iptc.org/about-iptc/][Global Standards Body]] of the News Media industry. 
[[https://iptc.org/standards/media-topics/][IPTC Media Topics]] is a list of 1100 [[http://cv.iptc.org/newscodes/mediatopic][Media Topics]] developed as an extension of the earlier  [[http://cv.iptc.org/newscodes/subjectcode][Subject Codes]].
You can [[http://show.newscodes.org/index.html?newscodes%3Dmedtop&lang%3Den-GB&startTo%3DShow][explore them interactively]] and will see that Multisensor "categories" are similar to these topics/subjects; 
and we'll continue to map them to ~dc:subject~.

On the other hand, Multisensor "keywords" are free keywords that describe a lot more specific things.
We'll map them to ~schema:keywords~, defined as 
"Keywords or tags used to describe this content. Multiple entries in a keywords list are typically delimited by commas":
#+BEGIN_SRC Turtle
dc:subject "Economy, Business & Finance";
schema:keywords "Europaische Union, ISIN_FR0003500008, Brexit, Erholungskurs, Europa, Paris, Grossbritannien";
#+END_SRC

It would be nice to:
- Split both categories and keywords on ", " and " & " and emit as separate values
- Map our categories to IPTC Media Topics

** Entity Alignment Service [0/5]
<2016-06-28 Tue>: checked [[http://grinder1.multisensorproject.eu/cepfiles/rdf_validation/entity_alignment/0181e1-ENTITY_ALIGNMENT-21-6-2016.ttl][0181e1-ENTITY_ALIGNMENT-21-6-2016.ttl]] and alignment.log (by email)

The log has 90 actions. I checked these 4 actions:
#+BEGIN_EXAMPLE
2016-06-21 16:22:18 INFO  Alignment:42 - Comparing <#char=1453,1461> and <#char=1444,1461>
2016-06-21 16:22:18 INFO  Alignment:138 - Removed: (#char=1453,1461, rdf:type, nif:Phrase)
2016-06-21 16:22:18 INFO  Alignment:152 - Removed: (#char=1453,1461, itsrdf:taClassRef, null)
2016-06-21 16:22:18 INFO  Alignment:156 - Removed: (#char=1453,1461, itsrdf:taIdentRef, null)
#+END_EXAMPLE

This corresponds to two annotations:
- <#char=1444,1461> found by Named Entity Recognition: "Margaret Thatcher" detected as the politician, with link to DBpedia (longer; correct)
- <#char=1453,1461> found by Entity Linking: "Thatcher" detected as a "roof builder" with link to Bbelnet (shorter; incorrect)

The Entity Alignment service prefers the longer annotation, and removes 3 properties from the shorter annotation.
What is left in the RDF is this:
#+BEGIN_SRC Turtle
<#char=1453,1461>
        a                     nif:Word ;
        <http://persistence.uni-leipzig.org/nlp2rdf/ontologies/nif-annotation#taIdentConf>
                "0.0"^^xsd:double ;
        <http://persistence.uni-leipzig.org/nlp2rdf/ontologies/nif-annotation#taIdentProv>
                <http://babelfy.org/> ;
        nif:anchorOf          "Thatcher" ;
        nif:beginIndex        "1453"^^xsd:nonNegativeInteger ;
        nif:endIndex          "1461"^^xsd:nonNegativeInteger ;
        nif:referenceContext  <#char=0,2898> .

<#char=1444,1461>
        a                     nif:Phrase ;
        nif:anchorOf          "Margaret Thatcher" ;
        nif:beginIndex        "1444"^^xsd:nonNegativeInteger ;
        nif:endIndex          "1461"^^xsd:nonNegativeInteger ;
        nif:referenceContext  <#char=0,2898> ;
        its:taClassRef        nerd:Person ;
        its:taIdentRef        dbr:Margaret_Thatcher .

dbr:Margaret_Thatcher
        a          foaf:Person , dbo:Person , nerd:Person ;
        foaf:name  "Margaret Thatcher" .
#+END_SRC

*** TODO Also remove taIdentConf, taIdentProv
In the example above, ~taClassRef~ and ~taIdentRef~ were removed. 
This makes the other two props ~nif-ann:taIdentConf~ and ~nif-ann:taIdentProv~ useless.
Remove them too.

*** TODO Leave Dependency Links
Entity Alignment also seems to remove the dependency links, eg:
: <#char=1444,1452> nif:dependency          <#char=1453,1461>
: <#char=1444,1452> upf-deep:deepDependency <#char=1453,1461>

However, this can make the dependency and FrameNet graphs disconnected. So leave the dependencies alone.

*** TODO Use Prefixes in alignment.log
I shortened the excerpt from alignment.log above to improve readability:
substituted the defined prefixes, and used the SIMMO URL as base (i.e. used relative URLs starting with hash)
It would be very useful if alignment.log uses the same shortenings to improve readability.

This is a completely cosmetic issue, we can cancel it.

** Summarization Service [0/4]
<2016-06-28 Tue> looked at [[http://grinder1.multisensorproject.eu/cepfiles/rdf_validation/concept_with_scores_16_06_2016/2c9d5c-CONCEPT_EXTRACTION-16-6-2016.ttl][2c9d5c-CONCEPT_EXTRACTION-16-6-2016.ttl]] (concept_with_scores)

#+BEGIN_SRC Turtle
<#char=0,11>
        a                        nif:Phrase ;
        <http://persistence.uni-leipzig.org/nlp2rdf/ontologies/nif-annotation#taIdentConf>
                "0.0"^^xsd:double ;
        <http://persistence.uni-leipzig.org/nlp2rdf/ontologies/nif-annotation#taIdentProv>
                <http://babelfy.org/> ;
        nif:beginIndex           "0"^^xsd:nonNegativeInteger ;
        nif:dependency           <#char=29,38> ;
        nif:endIndex             "11"^^xsd:nonNegativeInteger ;
        nif:lemma                "open_source" ;
        nif:literalAnnotation
          "surf=spos=NN" ,
          "rel==dpos=NN|end_string=11|start_string=0|id0=1|number=SG|word=open_source|connect_check=OK|vn=open_source" ,
          "deep=spos=NN" ;
        nif:oliaLink             upf-dep-syn:NAME , upf-deep:NAME , <#char=0,11_fe> , penn:NNP ;
        nif:referenceContext     <#char=0,5625> ;
        upf-deep:deepDependency  <#char=29,38> ;
        its:taClassRef           ms:GenericConcept ;
        its:taIdentRef           bn:s01157392n .
#+END_SRC

*** TODO Refresh Prefixes
I've added http://persistence.uni-leipzig.org/nlp2rdf/ontologies/nif-annotation# to [[./img/prefixes.ttl]] (committed <2016-06-30 Thu>)
Please refresh from [[./img/prefixes.ttl]], so new validation files use this prefix.

*** TODO nif:anchorOf or not?
I've been saying all along to skip ~nif:anchorOf~ so as not to create too many literals.
Above, UPF has done just that; while the other services emit it.

But with the number of SIMMOs loaded, it has not been too taxing for GDB.
~nif:anchorOf~ has been instrumental in debugging, eg of the UTF-8 and offset mismatch issues.

~nif:literalAnnotation~ and ~nif:lemma~ provide sufficient info about the phrase, 
so maybe we don't need ~nif:anchorOf~. We could cancel this issue.

*** TODO Why nif-ann:taIdentConf is 0?
In the above example, ~nif-ann:taIdentConf is 0. 
In many other examples it's a good number, eg see below.
Is 0 some sort of bug, or does Babelfy actually return 0 confidence for some concepts?

[[http://grinder1.multisensorproject.eu/cepfiles/rdf_validation/concept_with_scores_16_06_2016/bf6fe4-CONCEPT_EXTRACTION-16-6-2016.ttl][bf6fe4-CONCEPT_EXTRACTION-16-6-2016.ttl]]
#+BEGIN_SRC Turtle
@base <http://data.multisensorproject.eu/content/bf6fe48b8d88c1d11d5086863f4c3ad26286bda9>.

<#char=1814,1822>
        a                        nif:Word ;
        <http://persistence.uni-leipzig.org/nlp2rdf/ontologies/nif-annotation#taIdentConf>
                "0.7619547411890493"^^xsd:double ;
        <http://persistence.uni-leipzig.org/nlp2rdf/ontologies/nif-annotation#taIdentProv>
                <http://babelfy.org/> ;
        nif:anchorOf             "pastries" ;
        nif:beginIndex           "1814"^^xsd:nonNegativeInteger ;
        nif:dependency           <#char=1806,1812> ;
        nif:endIndex             "1822"^^xsd:nonNegativeInteger ;
        nif:lemma                "pastry" ;
        nif:literalAnnotation
          "deep=spos=NN" , 
          "rel==member=A2|dpos=NN|end_string=1822|start_string=1814|id0=29|word=pastry|number=PL|connect_check=OK|fn=Food" , 
          "surf=spos=NN" ;
        nif:oliaLink             upf-deep:COORD , penn:NNS , <#char=1814,1822_fe> , upf-dep-syn:COORD ;
        nif:referenceContext     <#char=0,12793> ;
        upf-deep:deepDependency  <#char=1806,1812> ;
        its:taClassRef           ms:GenericConcept ;
        its:taIdentRef           bn:s00060957n .
#+END_SRC

*** TODO ms:GenericConcept vs ms:SpecificConcept
Gerard wrote about the last example: 'generic' concept produced by Babelfy. 
Annotations of concepts produced by the concept extraction service should contain triples pointing to ~ms:SpecificConcept~.

It also seems to me that concepts like "open source" and "pastry" are ~ms:SpecificConcept~.

*** TODO Optimize Summarization Queries
Gerard wrote some of the Summarization queries are slow.
Please mark which ones need optimization, and provide ~$graph~ for testing.

- I made tentative new variant for 8.6 "Get Specific Concepts"
- Used the standard notation ~$param~ to indicate an input parameter, rather than ~__PARAM__~
- Moved ~FILTER~ inside ~GRAPH~, and a few more minor changes

** Content Alignment
The Content Alignment Pipeline (CAP) is a service that executes on KB data and finds articles that are similar or contradictory to the source article.
It is *not* executed as part of the SIMMO pipeline, but periodically.

<2016-06-28 Tue> checked [[http://mklab2.iti.gr/multisensor/index.php/CAP:_Specification_of_the_service][CAP:_Specification_of_the_service]]. It proposes the following model:

#+BEGIN_SRC Turtle
<http://data.multisensorproject.eu/content/53a0938bc4770c6ba0e7d7b9ca88a637f9e9c304#similarArticles>
  a oa:Annotation ;
  oa:hasTarget <http://data.multisensorproject.eu/content/53a0938bc4770c6ba0e7d7b9ca88a637f9e9c304> ;
  oa:hasBody        
    <http://data.multisensorproject.eu/content/53a0938bc4770c6ba0e7d7b9ca88a637f9e9c304#similarArticle-1> ,
    <http://data.multisensorproject.eu/content/53a0938bc4770c6ba0e7d7b9ca88a637f9e9c304#similarArticle-2> ;
  oa:motivatedBy oa:tagging ;
  oa:annotatedBy <http://data.multisensorproject.eu/agent/CAPAgent> ;
  oa:annotatedAt "2016-01-11T12:00:00"^^xsd:dateTime .

<http://data.multisensorproject.eu/content/53a0938bc4770c6ba0e7d7b9ca88a637f9e9c304#similarArticle-1>
  a oa:SemanticTag ;
  skos:related <http://data.multisensorproject.eu/content/ca34bb35770bfa55434a0689d64e1e6a60611047> ;
  fise:confidence 0.862 .

<http://data.multisensorproject.eu/content/53a0938bc4770c6ba0e7d7b9ca88a637f9e9c304#similarArticle-2>
  a oa:SemanticTag ;
  skos:related <http://data.multisensorproject.eu/content/57e07befbda355c2eca2ee521926071ee9f5c719> ;
  fise:confidence 0.795 .

<http://data.multisensorproject.eu/agent/CAPAgent>
  a prov:SoftwareAgent ;
  foaf:name "Content Alignment Pipeline v1.0" .
#+END_SRC

Each annotation is symmetric, so it's written twice: in the SIMMO graphs of each of the two SIMMOs.
This complicates data management, because both of these graphs need to be updated.

*** TODO One Annotation Per Pair                                    :Babis:
After consultation with Babis, we decided to change the representation as follows:
- Write annotations in their own graph <http://data.multisensorproject.eu/CAP>, outside of any SIMMO graph.
  The CAP service will be called periodically, search globally in the SIMMO DB, and overwrite the similarity graph.
- Write one annotation per pair
- Use custom ~oa:motivatedBy~: ~ms:linking-similar~ vs ~ms:linking-contradictory~ to express similarity vs contradiction

In the previous example, assume that the first related article is *similar* but the second is *contradictory*.
We restructure it as follows, where ~similarity/123~ and ~similarity/124~ are GUIDs or some other way to generate unique URLs.
Please note that the representation is completely symmetric regarding the two SIMMOs being linked, 
so there's no need to repeat for the other SIMMO.

#+BEGIN_SRC Turtle
graph <http://data.multisensorproject.eu/CAP> {
  <http://data.multisensorproject.eu/CAP/123> a oa:Annotation;
    oa:hasBody        
      <http://data.multisensorproject.eu/content/53a0938bc4770c6ba0e7d7b9ca88a637f9e9c304>,
      <http://data.multisensorproject.eu/content/ca34bb35770bfa55434a0689d64e1e6a60611047>;
    fise:confidence 0.862;
    oa:motivatedBy ms:linking-similar;
    oa:annotatedBy <http://data.multisensorproject.eu/agent/CAP>;
    oa:annotatedAt "2016-01-11T12:00:00"^^xsd:dateTime .

  <http://data.multisensorproject.eu/CAP/124> a oa:Annotation;
    oa:hasBody        
      <http://data.multisensorproject.eu/content/53a0938bc4770c6ba0e7d7b9ca88a637f9e9c304>,
      <http://data.multisensorproject.eu/content/57e07befbda355c2eca2ee521926071ee9f5c719>;
    fise:confidence 0.795;
    oa:motivatedBy ms:linking-contradictory;
    oa:annotatedBy <http://data.multisensorproject.eu/agent/CAP>;
    oa:annotatedAt "2016-01-12T12:00:00"^^xsd:dateTime .
}
#+END_SRC

*** TODO Use ms:score not fise:confidence for CAP
In an example sent by Babis, I see fise:confidence=1.6439653807554948. 
But confidence is the probability that something is true, so it should be <=1.
Guess this is some other sort of *score*, and maybe it's better to use our own property?

Decided with Babis to use a custom property ~ms:score~ (see next).

*** TODO Add to Ontology                                         :Vladimir:
The following will be in [[./img/ontology.ttl]], so they don't need to be repeated by CAP for every similarity link.
#+BEGIN_SRC Turtle
<http://data.multisensorproject.eu/agent/CAP> a prov:SoftwareAgent;
  foaf:name "Content Alignment Pipeline v1.0".

ms:linking-similar a owl:NamedIndividual, oa:Motivation;
  skos:inScheme oa:motivationScheme;
  skos:broader oa:linking;
  skos:prefLabel "linking-similar"@en;
  rdfs:comment "Motivation that represents a symmetric link between two *similar* articles"@en;
  rdfs:isDefinedBy ms: .

ms:linking-contradictory a owl:NamedIndividual, oa:Motivation;
  skos:inScheme oa:motivationScheme;
  skos:broader oa:linking;
  skos:prefLabel "linking-contradictory"@en;
  rdfs:comment "Motivation that represents a symmetric link between two *contradictory* articles"@en;
  rdfs:isDefinedBy ms: .

ms:score a owl:DatatypeProperty;
  rdfs:domain oa:Annotation;
  rdfs:range  xsd:decimal;
  rdfs:label "score"@en;
  rdfs:comment "Strength of an Annotation, eg the link between two entities"@en;
  rdfs:isDefinedBy ms: .
#+END_SRC

*** CAP Query
Given a ~$simmo~, find similar or contradictory articles, and their similarity/contradiction scores.
#+BEGIN_SRC sparql
select ?article ?motivation ?score {
  [a oa:Annotation;
   oa:annotatedBy <http://data.multisensorproject.eu/agent/CAP>;
   oa:hasBody $simmo, ?article;
   ms:score ?score;
   oa:motivatedBy ?motivation
  ]
  filter ($simmo != ?article)
}
#+END_SRC

*** TODO Other CAP Queries
The gdoc maybe has 2 queries related to CAP. Not sure I'm looking at the right section. Maybe we should just delete them.
- 2.8 "Retrieve the concepts in the SIMMO (Select)": wrote something simple
- 2.9 "Retrieve the concepts in the SIMMO (Construct)": don't know what is needed
