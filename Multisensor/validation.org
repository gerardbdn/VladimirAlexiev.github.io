#+TITLE: Multisensor Validation Log
#+DATE: <2016-07-06>
#+AUTHOR: Vladimir Alexiev
#+EMAIL: vladimir.alexiev@ontotext.com
#+OPTIONS: ':nil *:t -:t ::t <:t H:5 \n:nil ^:{} arch:headline author:t c:nil
#+OPTIONS: creator:comment d:(not "LOGBOOK") date:t e:t email:nil f:t inline:t num:nil
#+OPTIONS: p:nil pri:nil stat:t tags:t tasks:t tex:t timestamp:nil toc:t todo:nil |:t
#+CREATOR: Emacs 25.0.50.1 (Org mode 8.2.10)
#+DESCRIPTION:
#+EXCLUDE_TAGS: noexport
#+KEYWORDS:
#+LANGUAGE: en
#+SELECT_TAGS: export
#+TODO: TODO QUE | DONE CANCEL

[[https://github.com/VladimirAlexiev/VladimirAlexiev.github.io/blob/master/Multisensor/validation.org][Github Source]], [[http://VladimirAlexiev.github.io/Multisensor/validation.html][HTML Rendered version]]


* Table of Contents                                            :TOC:noexport:
 - [[#intro][Intro]]
   - [[#queries][Queries]]
 - [[#validation][Validation]]
   - [[#context][Context]]
     - [[#crawler-to-decode-html-entities][Crawler to decode HTML entities]]
     - [[#keywords-vs-category][Keywords vs Category]]
     - [[#ingest-timestamp][Ingest Timestamp]]
     - [[#simmo-quality][SIMMO Quality]]
     - [[#missing-authors][Missing Authors]]
     - [[#genre-type][Genre (Type)]]
   - [[#entity-linking-service][Entity Linking Service]]
     - [[#underscores-to-spaces][Underscores to Spaces]]
   - [[#advanced-context-extraction-service][Advanced Context Extraction Service]]
     - [[#wrong-prefix-for-text-characteristics][Wrong prefix for Text Characteristics]]
   - [[#entity-alignment-service][Entity Alignment Service]]
     - [[#also-remove-taidentconf-taidentprov][Also remove taIdentConf, taIdentProv]]
     - [[#leave-dependency-links][Leave Dependency Links]]
     - [[#use-prefixes-in-alignmentlog][Use Prefixes in alignment.log]]
   - [[#summarization-service][Summarization Service]]
     - [[#refresh-prefixes][Refresh Prefixes]]
     - [[#nifanchorof][nif:anchorOf]]
     - [[#why-nif-anntaidentconf-is-0][Why nif-ann:taIdentConf is 0?]]
     - [[#msgenericconcept-vs-msspecificconcept][ms:GenericConcept vs ms:SpecificConcept]]
     - [[#optimize-summarization-queries][Optimize Summarization Queries]]
   - [[#content-alignment][Content Alignment]]
     - [[#one-annotation-per-pair][One Annotation Per Pair]]
     - [[#use-msscore-not-fiseconfidence-for-cap][Use ms:score not fise:confidence for CAP]]
     - [[#add-to-ontology][Add to Ontology]]
     - [[#cap-query][CAP Query]]
     - [[#other-cap-queries][Other CAP Queries]]

* Intro
From now until the end of the project I'll keep a detailed log of what I validated and defects I found.
I'll use Org-mode mechanisms to track the defects (tags TODO, DONE, CANCELED) and try to monitor email conversation to keep this up to date.
The numbers in brackets after a section name show the resolved vs total defects.
Whenever a defect is posted in Jira, I'll track the issue number.

Please contact me on skype://valexiev1 for corrections/additions.
Even better, you could [[https://github.com/VladimirAlexiev/VladimirAlexiev.github.io/edit/master/Multisensor/validation.org][edit this file on github]] (it's a plain text file!) and send me a pull request.
- Mistakes or imprecise info (eg the scope of a particular service is described wrong, or a particular issue is in fact a non-issue)
- When an issue is resolved
- When an issue should be canceled
I'll also attend weekly calls related to RDF data and validation, and update this.

IMPORTANT: Just because an issue is listed in the section for a particular service, doesn't necessarily mean that service created the defect!

** Queries
Below I also track questions related to queries.
But when agreed, they should be moved to gdoc [[https://docs.google.com/document/d/1FfkiiTYvrLzHJ5P5j34NRVGPbXml0ndpNtiNbH2osRw/edit][Multisensor SPARQL Queries]].

Or we could simply use the doc:
if you need a new query, or a query needs optimization, please write a comment in red,
and notify me in a gdoc comment (add +vladimir@sirma.bg to the comment).

* Validation

** Context

*** TODO Crawler to decode HTML entities
It would be good if the crawler decodes HTML entities before storing in dc:subject (possibly also dc:title, dc:description). Eg:
#+BEGIN_SRC Turtle
dc:subject
  "Europ&auml;ische Union, ISIN_FR0003500008, Brexit, Erholungskurs, Europa, Paris, Gro&szlig;britannien" ,
  "Economy, Business & Finance" ;
#+END_SRC

*** DONE Keywords vs Category
Victor: both of the following fields extracted by the crawler are mapped to dc:subject:
- *category*: eg "Economy, Business & Finance" 
- *keywords*: eg "Europaische Union, ISIN_FR0003500008, Brexit, Erholungskurs, Europa, Paris, Grossbritannien"
Should we separate them in different properties?

Vladimir: 
[[https://iptc.org][IPTC]] is the [[https://iptc.org/about-iptc/][Global Standards Body]] of the News Media industry. 
[[https://iptc.org/standards/media-topics/][IPTC Media Topics]] is a list of 1100 [[http://cv.iptc.org/newscodes/mediatopic][Media Topics]] developed as an extension from earlier IPTC [[http://cv.iptc.org/newscodes/subjectcode][Subject Codes]].
You can [[http://show.newscodes.org/index.html?newscodes%3Dmedtop&lang%3Den-GB&startTo%3DShow][explore interactively]] and will see that Multisensor *categories* are similar to these topics/subjects; 
so we'll continue to map them to ~dc:subject~.

On the other hand, Multisensor *keywords* are free keywords that describe a lot more specific things.
We'll map them to ~schema:keywords~, defined as 
"Keywords or tags used to describe the content. Multiple entries in a keywords list are typically delimited by commas":
#+BEGIN_SRC Turtle
dc:subject "Economy, Business & Finance";
schema:keywords "Europaische Union, ISIN_FR0003500008, Brexit, Erholungskurs, Europa, Paris, Grossbritannien";
#+END_SRC

It would be nice to:
- Split keywords on ", " and and emit as separate values
  - (don't split categories, since the 3 words really represent one category)
- Map our categories to IPTC Media Topics. This is quite harder

*** TODO Ingest Timestamp
Victor: introduce the date when the article have been processed in RDF. 
In order to keep track of which and when the “curated selected data” have been processed,
and match them with the current version of the CEP service.

Vladimir: we can use ~dct:issued~ for this purpose:
#+BEGIN_SRC Turtle
dc:date    "2016-06-20T18:45:07.000+02:00"^^xsd:dateTime ; # date crawled
dct:issued "2016-06-30T12:34:56.000+02:00"^^xsd:dateTime ; # date processed by pipeline and ingested to GDB
#+END_SRC
But who will make this second timestamp? 

*** TODO SIMMO Quality
Victor: the field "c_quality" is sent now. Values can be:
- 0 = no quality assigned
- 1 = high quality
- 2 = medium quality
- 3 = low quality
- 5 = curated
Vladimir:
- The scale should be reordered to be monotonically increasing
- Maybe instead of 0, we should omit the statement
- Is there a valuel 4?
- Do we have metadata who & when curated it?

Vladimir: I searched for a [[http://lov.okfn.org/dataset/lov/terms?q%3Dquality&type%3Dproperty&page%3D2][quality property on LOV]], couldn't find anything really appropriate.
- http://www.w3.org/ns/dcat#dataQuality: this is about datasets, but is deprecated: 
  "This should not be used to describe the data collection characteristics, other more specialized statistical properties can be used instead". 
  But I don't see such statistical properties
- http://def.seegrid.csiro.au/isotc211/iso19115/2003/metadata#dataQualityInfo:
  this is about ISO 19115 "Geographic information — metadata"
  http://def.seegrid.csiro.au/isotc211/iso19115/2003/dataquality is a whole separate module on Quality
- http://purl.oclc.org/NET/ssnx/ssn#qualityOfObservation: this is about Semantic Sensor Networks.
  It makes reference to resultQuality in [[http://portal.opengeospatial.org/files/?artifact_id%3D41579][ISO 19156]] "Geographic information — Observations and measurements"

Finally I found the W3C [[https://www.w3.org/TR/vocab-dqv/][Data Quality Vocabulary]] ~dqv:~. We'll use [[https://www.w3.org/TR/vocab-dqv/#DimensionsofZaveri][Linked Data Quality Dimensions]] ~ldqd:~ by Zaveri.
First we add a ~dqv:Metric~ to the Multisensor ontology:
#+BEGIN_SRC Turtle
@prefix dqv:  <http://www.w3.org/ns/dqv#> .
@prefix ldqd: <http://www.w3.org/2016/05/ldqd#> .

ms:accuracy a dqv:Metric;
  skos:prefLabel "Accuracy"@en;
  skos:definition """Degree to which SIMMO data correctly represents the real world facts:
1=low, 2=medium, 3=high, 5=manually curated"""@en ;
  dqv:inDimension ldqd:semanticAccuracy;
  dqv:expectedDataType xsd:integer.
#+END_SRC

Then for each SIMMO that has a quality rating (those that don't have a rating get no extra statements):
#+BEGIN_SRC Turtle
ms-content:b3f35 dqv:hasQualityMeasurement ms-content:b3f35-quality.

ms-content:b3f35-quality a dqv:QualityMeasurement ;
   dqv:isMeasurementOf ms:accuracy; dqv:value 3.
#+END_SRC

*** TODO Missing Authors
#+BEGIN_SRC sparql
select * {?x a foaf:Document} 
# 112k SIMMOs
select * {?x a foaf:Document; dc:creator ?y}
# 10.5k authors, only 9.4%
#+END_SRC

*** TODO Genre (Type)
#+BEGIN_SRC sparql
select * {?x a foaf:Document; dc:type ?y} 
# 20k, that's 17.9%
#+END_SRC

Distribution of Genre:
#+BEGIN_SRC sparql
select ?y (count(*) as ?c)
{?x a foaf:Document; dc:type ?y}
group by ?y order by desc(?c)
#+END_SRC
| Genre/Type        | Count | Notes                  |
|-------------------+-------+------------------------|
| article           | 14768 |                        |
| music             |  2886 |                        |
| website           |  1087 |                        |
| speech            |   813 |                        |
| sound             |   407 |                        |
| food              |    83 | ?? Maybe "recipe"      |
| video             |    25 |                        |
| Article           |    25 | normalize to "article" |
| single            |    11 |                        |
| song              |    11 |                        |
| Speech            |    11 |                        |
| Ogg               |    11 |                        |
| video.other       |     7 | normalize to "video"   |
| news              |     6 |                        |
| ARTICLE           |     4 | normalize to "article" |
| media             |     2 |                        |
| blog              |     1 |                        |
| slideshow         |     1 |                        |
| video.movie       |     1 | normalize to "video"   |
| tumblr-feed:entry |     1 |                        |

** Entity Linking Service

*** TODO Underscores to Spaces
The EL service emits Babelnet entity labels in up to 4 languages, eg
#+BEGIN_SRC Turtle
bn:s00088614v  skos:prefLabel  "zu_befriedigen"@de , "satisfacer"@es , "satisfaire"@fr , "задоволи"@bg .
bn:s00014609n  skos:prefLabel  "Kuchen"@de , "Pastel_(gastronomía)"@es , "Gâteau"@fr , "Торта"@bg .
bn:s01718102n  skos:prefLabel  "I_do_not_want_what_I_haven't_got"@es , "I_Don't_Want_What_I_Haven't_Got"@en , "I_Do_Not_Want_What_I_Haven't_Got"@fr .
bn:s02229586n  skos:prefLabel  "UHC_Hamburg"@en , "Uhlenhorster_HC"@fr .
#+END_SRC
For reasons unknown, Babelnet uses underscores (eg see [[http://babelnet.org/rdf/page/UHC_Hamburg_n_EN][UHC_Hamburg_n_EN]]).
I think EL should convert the underscores to spaces to make the label more natural.

Should we also remove disambiguations, which are trailing parenthesized parts, eg "Pastel_(gastronomía)" -> "Pastel"?
Since these labels are not used for NLP tasks, and the disambiguations are very useful for understanding what the entity is, let's leave them.

** Advanced Context Extraction Service
<2016-06-23 Thu>: checked [[http://grinder1.multisensorproject.eu/cepfiles/rdf_validation/context_extraction_validation/0e6b24-CONTEXT_EXTRACTION-22-6-2016.ttl][0e6b24-CONTEXT_EXTRACTION-22-6-2016.ttl]]

This service adds new Text Characteristics properties (technicality, fluency, richness) to the context.

*** DONE Wrong prefix for Text Characteristics
Text Characteristics (technicality, fluency, richness):
- currently use <http://data.multisensor.eu/ontology#>
- but the correct prefix is <http://data.multisensorproject.eu/ontology#>

Victor <2016-06-30 Thu>: updated the prefix

** Entity Alignment Service
<2016-06-28 Tue>: checked [[http://grinder1.multisensorproject.eu/cepfiles/rdf_validation/entity_alignment/0181e1-ENTITY_ALIGNMENT-21-6-2016.ttl][0181e1-ENTITY_ALIGNMENT-21-6-2016.ttl]] and alignment.log (by email)

The log has 90 actions. I checked these 4 actions:
#+BEGIN_EXAMPLE
2016-06-21 16:22:18 INFO  Alignment:42 - Comparing <#char=1453,1461> and <#char=1444,1461>
2016-06-21 16:22:18 INFO  Alignment:138 - Removed: (#char=1453,1461, rdf:type, nif:Phrase)
2016-06-21 16:22:18 INFO  Alignment:152 - Removed: (#char=1453,1461, itsrdf:taClassRef, null)
2016-06-21 16:22:18 INFO  Alignment:156 - Removed: (#char=1453,1461, itsrdf:taIdentRef, null)
#+END_EXAMPLE

This corresponds to two annotations:
- <#char=1444,1461> found by Named Entity Recognition: "Margaret Thatcher" detected as the politician, with link to DBpedia (longer; correct)
- <#char=1453,1461> found by Entity Linking: "Thatcher" detected as a "roof builder" with link to Bbelnet (shorter; incorrect)

The Entity Alignment service prefers the longer annotation, and removes 3 properties from the shorter annotation.
What is left in the RDF is this:
#+BEGIN_SRC Turtle
<#char=1453,1461>
        a                     nif:Word ;
        <http://persistence.uni-leipzig.org/nlp2rdf/ontologies/nif-annotation#taIdentConf>
                "0.0"^^xsd:double ;
        <http://persistence.uni-leipzig.org/nlp2rdf/ontologies/nif-annotation#taIdentProv>
                <http://babelfy.org/> ;
        nif:anchorOf          "Thatcher" ;
        nif:beginIndex        "1453"^^xsd:nonNegativeInteger ;
        nif:endIndex          "1461"^^xsd:nonNegativeInteger ;
        nif:referenceContext  <#char=0,2898> .

<#char=1444,1461>
        a                     nif:Phrase ;
        nif:anchorOf          "Margaret Thatcher" ;
        nif:beginIndex        "1444"^^xsd:nonNegativeInteger ;
        nif:endIndex          "1461"^^xsd:nonNegativeInteger ;
        nif:referenceContext  <#char=0,2898> ;
        its:taClassRef        nerd:Person ;
        its:taIdentRef        dbr:Margaret_Thatcher .

dbr:Margaret_Thatcher
        a          foaf:Person , dbo:Person , nerd:Person ;
        foaf:name  "Margaret Thatcher" .
#+END_SRC

*** DONE Also remove taIdentConf, taIdentProv
In the example above, ~taClassRef~ and ~taIdentRef~ were removed. 
This makes the other two props ~nif-ann:taIdentConf~ and ~nif-ann:taIdentProv~ useless.
Remove them too.

*** DONE Leave Dependency Links
Entity Alignment also seems to remove the dependency links, eg:
: <#char=1444,1452> nif:dependency          <#char=1453,1461>
: <#char=1444,1452> upf-deep:deepDependency <#char=1453,1461>

However, this can make the dependency and FrameNet graphs disconnected. So leave the dependencies alone.

*** TODO Use Prefixes in alignment.log
I shortened the excerpt from alignment.log above to improve readability:
substituted the defined prefixes, and used the SIMMO URL as base (i.e. used relative URLs starting with hash)
It would be very useful if alignment.log uses the same shortenings to improve readability.

This is a completely cosmetic issue, we can cancel it.

** Summarization Service
<2016-06-28 Tue> looked at [[http://grinder1.multisensorproject.eu/cepfiles/rdf_validation/concept_with_scores_16_06_2016/2c9d5c-CONCEPT_EXTRACTION-16-6-2016.ttl][2c9d5c-CONCEPT_EXTRACTION-16-6-2016.ttl]] (concept_with_scores)

#+BEGIN_SRC Turtle
<#char=0,11>
        a                        nif:Phrase ;
        <http://persistence.uni-leipzig.org/nlp2rdf/ontologies/nif-annotation#taIdentConf>
                "0.0"^^xsd:double ;
        <http://persistence.uni-leipzig.org/nlp2rdf/ontologies/nif-annotation#taIdentProv>
                <http://babelfy.org/> ;
        nif:beginIndex           "0"^^xsd:nonNegativeInteger ;
        nif:dependency           <#char=29,38> ;
        nif:endIndex             "11"^^xsd:nonNegativeInteger ;
        nif:lemma                "open_source" ;
        nif:literalAnnotation
          "surf=spos=NN" ,
          "rel==dpos=NN|end_string=11|start_string=0|id0=1|number=SG|word=open_source|connect_check=OK|vn=open_source" ,
          "deep=spos=NN" ;
        nif:oliaLink             upf-dep-syn:NAME , upf-deep:NAME , <#char=0,11_fe> , penn:NNP ;
        nif:referenceContext     <#char=0,5625> ;
        upf-deep:deepDependency  <#char=29,38> ;
        its:taClassRef           ms:GenericConcept ;
        its:taIdentRef           bn:s01157392n .
#+END_SRC

*** TODO Refresh Prefixes
I've added http://persistence.uni-leipzig.org/nlp2rdf/ontologies/nif-annotation# to [[./img/prefixes.ttl]] (committed <2016-06-30 Thu>)
Please refresh from [[./img/prefixes.ttl]], so new validation files use this prefix.

Gerard: what exactly are we suppposed to do with the prefixes file.
- Once loaded in the repo (Boyan's job), we can make queries without mentioning the prefixes.
Should we load it into Sesame somehow so that triples are generated with prefixes? 
If so, could you give us some code showing how to do it?
- I think the validation files use prefixes, because prefixes.ttl is prepended, then passed through RIOT.
  I think Victor made that script. The other prefixes are there, so it's just a matter of refreshing

*** TODO nif:anchorOf
I've been saying all along to skip ~nif:anchorOf~ so as not to create too many literals.
Above, UPF has done just that; while the other services emit it.

But with the number of SIMMOs loaded, it has not been too taxing for GDB.
~nif:anchorOf~ has been instrumental in debugging, eg of the UTF-8 and offset mismatch issues.

~nif:literalAnnotation~ and ~nif:lemma~ provide sufficient info about the phrase, 
so maybe we don't need ~nif:anchorOf~. We could cancel this issue.

Gerard: If they can be sustained by GraphDB, I vote in favor of keeping them as they help *a lot* when debugging. 

Vladimir: so decided: if the Entity Lookup makes a new node, add anchorOf to it.

*** TODO Why nif-ann:taIdentConf is 0?
In the above example, ~nif-ann:taIdentConf is 0. 
In many other examples it's a good number, eg see below.
Is 0 some sort of bug, or does Babelfy actually return 0 confidence for some concepts?

[[http://grinder1.multisensorproject.eu/cepfiles/rdf_validation/concept_with_scores_16_06_2016/bf6fe4-CONCEPT_EXTRACTION-16-6-2016.ttl][bf6fe4-CONCEPT_EXTRACTION-16-6-2016.ttl]]
#+BEGIN_SRC Turtle
@base <http://data.multisensorproject.eu/content/bf6fe48b8d88c1d11d5086863f4c3ad26286bda9>.

<#char=1814,1822>
        a                        nif:Word ;
        <http://persistence.uni-leipzig.org/nlp2rdf/ontologies/nif-annotation#taIdentConf>
                "0.7619547411890493"^^xsd:double ;
        <http://persistence.uni-leipzig.org/nlp2rdf/ontologies/nif-annotation#taIdentProv>
                <http://babelfy.org/> ;
        nif:anchorOf             "pastries" ;
        nif:beginIndex           "1814"^^xsd:nonNegativeInteger ;
        nif:dependency           <#char=1806,1812> ;
        nif:endIndex             "1822"^^xsd:nonNegativeInteger ;
        nif:lemma                "pastry" ;
        nif:literalAnnotation
          "deep=spos=NN" , 
          "rel==member=A2|dpos=NN|end_string=1822|start_string=1814|id0=29|word=pastry|number=PL|connect_check=OK|fn=Food" , 
          "surf=spos=NN" ;
        nif:oliaLink             upf-deep:COORD , penn:NNS , <#char=1814,1822_fe> , upf-dep-syn:COORD ;
        nif:referenceContext     <#char=0,12793> ;
        upf-deep:deepDependency  <#char=1806,1812> ;
        its:taClassRef           ms:GenericConcept ;
        its:taIdentRef           bn:s00060957n .
#+END_SRC

*** TODO ms:GenericConcept vs ms:SpecificConcept
Gerard wrote about the last example: 'generic' concept produced by Babelfy. 
Annotations of concepts produced by the concept extraction service should contain triples pointing to ~ms:SpecificConcept~.

It also seems to me that concepts like "open source" and "pastry" are ~ms:SpecificConcept~.

*** DONE Optimize Summarization Queries
Gerard wrote some of the Summarization queries are slow.
Please mark which ones need optimization, and provide ~$graph~ for testing.
- Used the standard notation ~$param~ to indicate an input parameter, rather than ~__PARAM__~
- Moved ~FILTER~ inside ~GRAPH~, and a few more minor changes
- The problem was that the prop path ~p1?/p2~ is slow, since ~p1?~ connects *any* node to itself.
  Replaced with ~p1/p2|p2~, which is fast

** Content Alignment
The Content Alignment Pipeline (CAP) is a service that executes on KB data and finds articles that are similar or contradictory to the source article.
It is *not* executed as part of the SIMMO pipeline, but periodically.

<2016-06-28 Tue> checked [[http://mklab2.iti.gr/multisensor/index.php/CAP:_Specification_of_the_service][CAP:_Specification_of_the_service]]. It proposes the following model:

#+BEGIN_SRC Turtle
<http://data.multisensorproject.eu/content/53a0938bc4770c6ba0e7d7b9ca88a637f9e9c304#similarArticles>
  a oa:Annotation ;
  oa:hasTarget <http://data.multisensorproject.eu/content/53a0938bc4770c6ba0e7d7b9ca88a637f9e9c304> ;
  oa:hasBody        
    <http://data.multisensorproject.eu/content/53a0938bc4770c6ba0e7d7b9ca88a637f9e9c304#similarArticle-1> ,
    <http://data.multisensorproject.eu/content/53a0938bc4770c6ba0e7d7b9ca88a637f9e9c304#similarArticle-2> ;
  oa:motivatedBy oa:tagging ;
  oa:annotatedBy <http://data.multisensorproject.eu/agent/CAPAgent> ;
  oa:annotatedAt "2016-01-11T12:00:00"^^xsd:dateTime .

<http://data.multisensorproject.eu/content/53a0938bc4770c6ba0e7d7b9ca88a637f9e9c304#similarArticle-1>
  a oa:SemanticTag ;
  skos:related <http://data.multisensorproject.eu/content/ca34bb35770bfa55434a0689d64e1e6a60611047> ;
  fise:confidence 0.862 .

<http://data.multisensorproject.eu/content/53a0938bc4770c6ba0e7d7b9ca88a637f9e9c304#similarArticle-2>
  a oa:SemanticTag ;
  skos:related <http://data.multisensorproject.eu/content/57e07befbda355c2eca2ee521926071ee9f5c719> ;
  fise:confidence 0.795 .

<http://data.multisensorproject.eu/agent/CAPAgent>
  a prov:SoftwareAgent ;
  foaf:name "Content Alignment Pipeline v1.0" .
#+END_SRC

Each annotation is symmetric, so it's written twice: in the SIMMO graphs of each of the two SIMMOs.
This complicates data management, because both of these graphs need to be updated.

*** TODO One Annotation Per Pair                                    :Babis:
After consultation with Babis, we decided to change the representation as follows:
- Write annotations in their own graph <http://data.multisensorproject.eu/CAP>, outside of any SIMMO graph.
  The CAP service will be called periodically, search globally in the SIMMO DB, and overwrite the similarity graph.
- Write one annotation per pair
- Use custom ~oa:motivatedBy~: ~ms:linking-similar~ vs ~ms:linking-contradictory~ to express similarity vs contradiction

In the previous example, assume that the first related article is *similar* but the second is *contradictory*.
We restructure it as follows, where ~similarity/123~ and ~similarity/124~ are GUIDs or some other way to generate unique URLs.
Please note that the representation is completely symmetric regarding the two SIMMOs being linked, 
so there's no need to repeat for the other SIMMO.

#+BEGIN_SRC Turtle
graph <http://data.multisensorproject.eu/CAP> {
  <http://data.multisensorproject.eu/CAP/123> a oa:Annotation;
    oa:hasBody        
      <http://data.multisensorproject.eu/content/53a0938bc4770c6ba0e7d7b9ca88a637f9e9c304>,
      <http://data.multisensorproject.eu/content/ca34bb35770bfa55434a0689d64e1e6a60611047>;
    fise:confidence 0.862;
    oa:motivatedBy ms:linking-similar;
    oa:annotatedBy <http://data.multisensorproject.eu/agent/CAP>;
    oa:annotatedAt "2016-01-11T12:00:00"^^xsd:dateTime .

  <http://data.multisensorproject.eu/CAP/124> a oa:Annotation;
    oa:hasBody        
      <http://data.multisensorproject.eu/content/53a0938bc4770c6ba0e7d7b9ca88a637f9e9c304>,
      <http://data.multisensorproject.eu/content/57e07befbda355c2eca2ee521926071ee9f5c719>;
    fise:confidence 0.795;
    oa:motivatedBy ms:linking-contradictory;
    oa:annotatedBy <http://data.multisensorproject.eu/agent/CAP>;
    oa:annotatedAt "2016-01-12T12:00:00"^^xsd:dateTime .
}
#+END_SRC

*** TODO Use ms:score not fise:confidence for CAP
In an example sent by Babis, I see fise:confidence=1.6439653807554948. 
But confidence is the probability that something is true, so it should be <=1.
Guess this is some other sort of *score*, and maybe it's better to use our own property?

Decided with Babis to use a custom property ~ms:score~ (see next).

*** TODO Add to Ontology                                         :Vladimir:
The following will be in [[./img/ontology.ttl]], so they don't need to be repeated by CAP for every similarity link.
#+BEGIN_SRC Turtle
<http://data.multisensorproject.eu/agent/CAP> a prov:SoftwareAgent;
  foaf:name "Content Alignment Pipeline v1.0".

ms:linking-similar a owl:NamedIndividual, oa:Motivation;
  skos:inScheme oa:motivationScheme;
  skos:broader oa:linking;
  skos:prefLabel "linking-similar"@en;
  rdfs:comment "Motivation that represents a symmetric link between two *similar* articles"@en;
  rdfs:isDefinedBy ms: .

ms:linking-contradictory a owl:NamedIndividual, oa:Motivation;
  skos:inScheme oa:motivationScheme;
  skos:broader oa:linking;
  skos:prefLabel "linking-contradictory"@en;
  rdfs:comment "Motivation that represents a symmetric link between two *contradictory* articles"@en;
  rdfs:isDefinedBy ms: .

ms:score a owl:DatatypeProperty;
  rdfs:domain oa:Annotation;
  rdfs:range  xsd:decimal;
  rdfs:label "score"@en;
  rdfs:comment "Strength of an Annotation, eg the link between two entities"@en;
  rdfs:isDefinedBy ms: .
#+END_SRC

*** CAP Query
Given a ~$simmo~, find similar or contradictory articles, and their similarity/contradiction scores.
#+BEGIN_SRC sparql
select ?article ?motivation ?score {
  [a oa:Annotation;
   oa:annotatedBy <http://data.multisensorproject.eu/agent/CAP>;
   oa:hasBody $simmo, ?article;
   ms:score ?score;
   oa:motivatedBy ?motivation
  ]
  filter ($simmo != ?article)
}
#+END_SRC

*** TODO Other CAP Queries
The gdoc maybe has 2 queries related to CAP. Not sure I'm looking at the right section. Maybe we should just delete them.
- 2.8 "Retrieve the concepts in the SIMMO (Select)": wrote something simple
- 2.9 "Retrieve the concepts in the SIMMO (Construct)": don't know what is needed
