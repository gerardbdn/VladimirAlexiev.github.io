-*- coding: utf-8; fill-column: 5000 -*-

@TechReport{Alexiev1993-annotatedBibliography,
  author       = {Vladimir Alexiev},
  title        = {{A (Not Very Much) Annotated Bibliography on Integrating Object-Oriented and Logic Programming}},
  institution  = {University of Alberta},
  month        = mar,
  year         = 1993,
  _url         = {ftp://ftp.cs.ualberta.ca/pub/oolog/oolog-bib.ps.gz},
  url          = {http://vladimiralexiev.github.io/pubs/Alexiev1993-annotatedBibliography.pdf},
}

@TechReport{Alexiev1993-mutableObjectState,
  author       = {Vladimir Alexiev},
  title        = {{Mutable Object State for Object-Oriented Logic Programming: A Survey}},
  number       = {TR93--15},
  institution  = {University of Alberta},
  month        = aug,
  year         = 1993,
  abstract     = {One of the most difficult problems on the way to an integration of Object-Oriented and Logic Programming is the modeling of changeable object state(i.e. object dynamics) in a particular logic in order not to forfeit the declarative nature of LP. Classical logic is largely unsuitable for such a task, because it adopts a general (both temporally and spatially), Platonic notion of validity, whereas object state changes over time and is local to an object. This paper presents the problem and surveys the state-of-the-art approaches to its solution, as well as some emerging, promising new approaches. The paper tries to relate the different approaches, to evaluate their merits and deficiencies and to identify promising directions for development. The emphasis in this survey is on efficient implementation of state change, one which would be suitable for the lowest fundamental level of a general OOLP language. The following approaches are covered: Assert/Retract, Declarative Database Updates and Transaction Logic, Modal and Dynamic Logics, Perpetual Objects, Logical Objects and Linear Objects, Linear Logic, Rewriting Logic and MaudeLog.},
  keywords     = {Object-Oriented Logic Programming, mutable object state, survey},
  url          = {http://vladimiralexiev.github.io/pubs/Alexiev1993-mutableObjectState.pdf},
  _url          = {ftp://ftp.cs.ualberta.ca/pub/TechReports/1995/TR93-15/},
}

@TechReport{Alexiev1993-objectOriented,
  author       = {Vladimir Alexiev},
  title        = {{Object-Oriented and Logic-Based Knowledge Representation}},
  institution  = {University of Alberta},
  howpublished = {Term project},
  year         = 1993,
  url          = {http://vladimiralexiev.github.io/pubs/Alexiev1993-objectOriented.pdf},
}

@Article{Alexiev1994-applicationsLinearLogic,
  author       = {Vladimir Alexiev},
  title        = {{Applications of Linear Logic to Computation: An Overview}},
  journal      = {Bulletin of the IGPL},
  volume       = 2,
  number       = 1,
  pages        = {77-107},
  month        = mar,
  year         = 1994,
  issn         = {0945-9103},
  note         = {Also University of Alberta TR93--18, December 1993},
  url          = {http://vladimiralexiev.github.io/pubs/Alexiev1994-applicationsLinearLogic.pdf},
  _url          = {ftp://ftp.cs.ualberta.ca/pub/TechReports/1993/TR93-18/},
  _url          = {ftp://ftp.theory.doc.ic.ac.uk/theory/forum/igpl/Bulletin/V2-1/Alexiev.ps.gz},
}

@TechReport{Alexiev1995-eventCalculus,
  author       = {Vladimir Alexiev},
  title        = {{The Event Calculus as a Linear Logic Program}},
  number       = {TR95--24},
  institution  = {University of Alberta},
  month        = sep,
  year         = 1995,
  abstract     = {The traditional presentation of Kowalski's Event Calculus as a logic program uses Negation- as-Failure (NAF) in an essential way to support persistence of fluents. In this paper we present an implementation of Event Calculus as a purely logical (without NAF) Linear Logic (LL) program. This work demonstrates some of the internal non-monotonic features of LL and its suitability for knowledge update (as opposed to knowledge revision). Although NAF is an ontologically sufficient solution to the frame problem, the LL solution is implementationally superior. Handling of incomplete temporal descriptions and support for ramifications (derived fluents) are also considered.},
  keywords     = {event calculus, linear logic, negation as failure, knowledge update},
  url          = {http://vladimiralexiev.github.io/pubs/Alexiev1995-eventCalculus.pdf},
  _url          = {ftp://ftp.cs.ualberta.ca/pub/TechReports/1995/TR95-24/TR95-24.ps.Z},
}

@TechReport{Alexiev1995-thesisProposal,
  author       = {Vladimir Alexiev},
  title        = {{Object-Oriented Logic Programming based on Linear Logic}},
  institution  = {University of Alberta},
  howpublised  = {Thesis proposal},
  month        = feb,
  year         = 1995
}

@InProceedings{Alexiev1996-targetedCommunication,
  author       = {Vladimir Alexiev},
  title        = {{Targeted Communication in Linear Objects}},
  booktitle    = {Artificial Intelligence: Methodology, Systems, Applications (AIMSA'96)},
  publisher    = {IOI Press},
  note         = {Also University of Alberta TR94--14},
  month        = sep,
  year         = 1996,
  abstract     = {Linear Objects (LO) of Andreoli and Pareschi is the first proposal to integrate object-oriented programming into logic programming based on Girard's Linear Logic (LL). In LO each object is represented by a separate open node of a proof tree. This ``insulates'' objects from one another which allows the attributes of an object to be represented as a multiset of atoms and thus facilitates easy retrieval and update of attributes. However this separation hinders communication between objects. Communication in LO is achieved through broadcasting to all objects which in our opinion is infeasible from a computational viewpoint. This paper proposes a refined communication mechanism for LO which uses explicit communication channels specified by the programmer. We name it TCLO which stands for ``Targeted Communication in LO''. Although channel specification puts some burden on the programmer, we demonstrate that the language is expressive enough by redoing some of the examples given for LO. Broadcasting can be done in a controlled manner. LO can be seen as a special case of TCLO where only one global channel (the forum) is used.},
  keywords     = {Linear Objects, communication, broadcasting, objects and logic, linear logic},
  url          = {http://vladimiralexiev.github.io/pubs/Alexiev1996-targetedCommunication.pdf},
  _url          = {ftp://ftp.cs.ualberta.ca/pub/TechReports/TR94-14/},
}

@TechReport{Alexiev1998-distributedSynchronization,
  author       = {Vladimir Alexiev},
  title        = {{Distributed Synchronization in a pi-Calculus with Bidirectional Communication}},
  institution  = {University of Alberta},
  month        = jan,
  year         = 1998,
  abstract     = {The (input) prefix operation of the pi-calculus expresses global synchronization (blocking) of the prefixed process. We show how to implement synchronization in a completely distributed manner, by using bidirectional atomic communication and the principle of provision (data-dependency-based synchronization)},
  keywords     = {pi-calculus, input prefix, distributed synchronization},
  url          = {http://vladimiralexiev.github.io/pubs/Alexiev1998-distributedSynchronization.pdf},
}

@TechReport{Alexiev1998-finitePi,
  author       = {Vladimir Alexiev},
  title        = {{Representing the Finite pi-calculus in Multi-Interaction Nets: Concurrency = Interaction + Non-determinism}},
  abstract     = {We extend the Interaction Nets of Lafont with some non-determinism capabilities and then show how to implement the finite monadic pi-calculus in that system},
  institution  = {University of Alberta},
  mon          = apr,
  year         = 1998,
  url          = {http://vladimiralexiev.github.io/pubs/Alexiev1998-finitePi.pdf},
}

@PhdThesis{Alexiev1999-thesis,
  author       = {Vladimir Alexiev},
  title        = {{Non-deterministic Interaction Nets}},
  school       = {University of Alberta},
  year         = 1999,
  url          = {http://vladimiralexiev.github.io/pubs/Alexiev1999-thesis.pdf},
  _url          = {http://vladimiralexiev.github.io/pubs/Alexiev1999-thesis-2up.pdf},
}

@Misc{Alexiev1999-thesisPresentation,
  author       = {Vladimir Alexiev},
  title        = {{Non-deterministic Interaction Nets (Thesis Presentation)}},
  howpublished = {Presentation},
  year         = 1999,
  url          = {http://vladimiralexiev.github.io/pubs/Alexiev1999-thesisPresentation.pdf},
}

@TechReport{Alexiev2004-DataIntegration,
  author       = {Vladimir Alexiev},
  title        = {{Data Integration Survey}},
  institution  = {European project "Corporate Ontology Grid" (COG)},
  type         = {Deliverable},
  month        = sep,
  year         = 2004,
}

@InProceedings{Alexiev2010-costEffectiveEGov,
  author       = {Vladimir Alexiev},
  title        = {{Cost-effective e-Government Services: Export Control System phase 2 (ECS2)}},
  booktitle    = {Bulgaria-Korea IT Experts Workshop},
  address      = {Sofia, Bulgaria},
  month        = feb,
  year         = 2010,
  url          = {http://vladimiralexiev.github.io/pubs/Alexiev2010-costEffectiveEGov.pdf},
}

@TechReport{Alexiev2011-KIM-Stanbol,
  author       = {Vladimir Alexiev},
  title        = {{Comparing Ontotext KIM and Apache Stanbol}},
  institution  = {Ontotext Corp},
  year         = 2011,
  month        = sep,
  type         = {Slideshare Presentation},
  url          = {http://www.slideshare.net/valexiev1/comparing-ontotext-kim-and-apache-stanbol},
 _url          = {http://www.slideshare.net/valexiev1/comparing-ontotext-kim-and-apache-stanbol-appendix}
}

@InProceedings{Alexiev2011-SemtechForCulturalHeritage,
  author       = {Vladimir Alexiev},
  title        = {{Semantic Technologies for Cultural Heritage}},
  booktitle    = {Global Smart SOC Initiative Summit},
  month        = may,
  year         = 2011,
  address      = {Seoul, Korea},
  url          = {http://vladimiralexiev.github.io/pubs/Alexiev2011-SemtechForCulturalHeritage.pdf},
}

@InProceedings{Alexiev2012-CRM-properties,
  author       = {Vladimir Alexiev},
  title        = {{Types and Annotations for CIDOC CRM Properties}},
  booktitle    = {Digital Presentation and Preservation of Cultural and Scientific Heritage (DiPP2012) conference (Invited report)},
  keywords     = {cultural heritage, CIDOC CRM, properties, attribute assignment, attribution, RDF reification, property reification},
  abstract     = {The CIDOC CRM provides an extensive ontology for describing entities and properties appearing in cultural heritage (CH) documentation, history and archeology. CRM provides some means for describing information about properties (property types, attribute assignment, and "long-cuts") and guidelines for extending the vocabulary.
However, these means are far from complete, and in some cases there is little guidance how to "implement" them in RDF. In this article we outline the prob-lems, relate them to established RDF patterns and mechanisms, and describe several implementation alternatives.},
  year         = 2012,
  month        = sep,
  address      = {Veliko Tarnovo, Bulgaria},
  url          = {http://vladimiralexiev.github.io/pubs/Alexiev2012-CRM-Properties.pdf},
  _url          = {http://vladimiralexiev.github.io/pubs/Alexiev2012-CRM-Properties-presentation.ppt},
}

@InProceedings{Alexiev2012-CRM-search,
  author       = {Vladimir Alexiev},
  title        = {{Implementing CIDOC CRM Search Based on Fundamental Relations and OWLIM Rules}},
  booktitle    = {Workshop on Semantic Digital Archives (SDA 2012), part of International Conference on Theory and Practice of Digital Libraries (TPDL 2012)},
  keywords     = {CIDOC CRM, cultural heritage, semantic search, Fundamental Concepts, Fundamental Relations},
  abstract     = {The CIDOC CRM provides an ontology for describing entities, properties and relationships appearing in cultural heritage (CH) documentation, history and archeology. CRM promotes shared understanding by providing an extensible semantic framework that any CH information can be mapped to. CRM data is usually represented in semantic web format (RDF) and comprises complex graphs of nodes and properties.
An important question is how a user can search through such complex graphs, since the number of possible combinations is staggering. One approach "com-presses" the semantic network by mapping many CRM entity classes to a few "Fundamental Concepts" (FC), and mapping whole networks of CRM proper-ties to fewer "Fundamental Relations" (FR). These FC and FRs serve as a "search index" over the CRM semantic web and allow the user to use a simpler query vocabulary.
We describe an implementation of CRM FR Search based on OWLIM Rules, done as part of the ResearchSpace (RS) project. We describe the technical de-tails, problems and difficulties encountered, benefits and disadvantages of using OWLIM rules, and preliminary performance results. We provide implementa-tion experience that can be valuable for further implementation, definition and maintenance of CRM FRs.},
  year         = 2012,
  month        = sep,
  address      = {Paphos, Cyprus},
  publisher    = {CEUR WS Vol.912},
  url          = {http://ceur-ws.org/Vol-912/paper8.pdf},
  _url          = {http://sda2012.dke-research.de/images/pdfs/crm-search-presentation.pdf},
}

@InProceedings{Alexiev2013-CRM-reasoning,
  author       = {Vladimir Alexiev and Dimitar Manov and Jana Parvanova and Svetoslav Petrov},
  title        = {{Large-scale Reasoning with a Complex Cultural Heritage Ontology (CIDOC CRM)}},
  booktitle    = {{Workshop Practical Experiences with CIDOC CRM and its Extensions (CRMEX 2013) at TPDL 2013}},
  abstract     = {The CIDOC Conceptual Reference Model (CRM) is an important ontology in the Cultural Heritage (CH) domain. CRM is intended mostly as a data integration mechanism, allowing reasoning and discoverability across diverse CH sources represented in CRM. CRM data comprises complex graphs of nodes and properties. An important question is how to search through such complex graphs, since the number of possible combinations is staggering. One answer is the "Fundamental Relations" (FR) approach that maps whole networks of CRM properties to fewer FRs, serving as a "search index" over the CRM semantic web. We present performance results for an FR Search implementation based on OWLIM. This search works over a significant CH dataset: almost 1B statements resulting from 2M objects of the British Museum. This is an exciting demonstration of large-scale reasoning with real-world data over a complex ontology (CIDOC CRM). We present volumetrics, hardware specs, compare the numbers to other repositories hosted by Ontotext, performance results, and compare performance of a SPARQL implementation.},
  keywords     = {CIDOC CRM, cultural heritage, semantic search, Fundamental Relations, OWLIM, semantic repository, inference, performance, benchmark},
  month        = sep,
  year         = 2013,
  address      = {Valetta, Malta},
  url          = {http://vladimiralexiev.github.io/pubs/Alexiev2013-CRM-reasoning.pdf},
  _url         = {http://vladimiralexiev.github.io/pubs/Alexiev2013-CRM-reasoning-slides.ppt},
}

@InProceedings{Alexiev2013-ResearchSpace,
  author       = {Vladimir Alexiev},
  title        = {{ResearchSpace as an Example of a VRE Based on CIDOC CRM}},
  booktitle    = {Virtual Center for Medieval Studies (Medioevo Europeo VCMS) Workshop},
  year         = 2013,
  month        = apr,
  address      = {Bucharest, Romania},
  url          = {http://www.slideshare.net/valexiev1/research-space-vre-based-on-cidoc-crm},
}

@InProceedings{Alexiev2014-Brussels,
  author       = {Vladimir Alexiev},
  title        = {{Semantic Technologies for Cultural Heritage}},
  booktitle    = {SmartCulture Conference},
  year         = 2014,
  month        = jun,
  address      = {Brussels, Belgium},
  url          = {http://vladimiralexiev.github.io/pres/20140611-SmartCulture-sem-tech-CH},
  _url         = {http://vladimiralexiev.github.io/pres/20140611-SmartCulture-sem-tech-CH/Semantic Technologies for Cultural Heritage.pdf},
}

@TechReport{Alexiev2014-ExtendingOWL2,
  author       = {Vladimir Alexiev},
  title        = {{Extending OWL2 Property Constructs with OWLIM Rules}},
  institution  = {Ontotext Corp},
  year         = 2014,
  number       = {(unpublished draft)},
  month        = sep,
  url          = {http://vladimiralexiev.github.io/pres/extending-owl2},
  keywords     = {OWL2, Property Chain Axiom, sub-property, property inferencing},
  abstract     = {While OWL2 has very powerful class constructs, its property constructs are quite weak. We propose several extensions that we found useful, and implement them using OWLIM rules},
}

@InProceedings{Alexiev2014-GVP-LOD,
  author       = {Vladimir Alexiev},
  title        = {{Getty Vocabulary Program LOD: Ontologies and Semantic Representation}},
  booktitle    = {CIDOC Congress},
  year         = 2014,
  month        = sep,
  address      = {Dresden, Germany},
  url          = {http://vladimiralexiev.github.io/pres/20140905-CIDOC-GVP},
  _url         = {http://vladimiralexiev.github.io/pres/20140905-CIDOC-GVP/GVP-LOD-CIDOC.pdf},
}

@Misc{Alexiev2014-GraphDBRuleProfiling,
  author       = {Vladimir Alexiev},
  title        = {{Ontotext GraphDB-SE Rule Profiling}},
  howpublished = {Product documentation},
  month        = dec,
  year         = 2014,
  url          = {https://confluence.ontotext.com/display/GraphDB65/GraphDB-SE+Rule+Profiling},
  abstract     = {GraphDB 6 includes a useful new feature that allows you to debug rule performance This page also includes Optimization Hints for ruleset performance.},
}

@InProceedings{Alexiev2014-LinguisticLD,
  author       = {Vladimir Alexiev},
  title        = {{Linguistic Linked Data}},
  booktitle    = {Multisensor Project Meeting},
  year         = 2014,
  month        = oct,
  address      = {Bonn, Germany},
  url          = {http://vladimiralexiev.github.io/Multisensor/20141008-Linguistic-LD},
  abstract     = {There's been a huge drive in recent years to represent NLP data as RDF. NLP data is usually large, so does it make sense to represent it as RDF? What's the benefit? Ontologies, schemas and groups include: GRaF ITS2 FISE LAF LD4LT LEMON LIME LMF MARL NERD NIF NLP2RDF OLIA OntoLex OntoLing OntoTag Penn Stanford... my oh my! There are a lot of linguistic resources available that can be used profitably: BabelNet FrameNet GOLD ISOcat LemonUBY Multitext OmegaNet UBY VerbNet Wiktionary WordNet.},
}

@InProceedings{Alexiev2014-Malmo,
  author       = {Vladimir Alexiev},
  title        = {{Semantic Technologies for Cultural Heritage}},
  booktitle    = {Malmo Linked Data Meetup},
  year         = 2014,
  month        = aug,
  address      = {Malmo, Sweden},
  url          = {http://vladimiralexiev.github.io/pres/20140821-Malmo},
  _url         = {http://vladimiralexiev.github.io/pres/20140821-Malmo/SemTechCH-Malmo.pdf},
}

@InProceedings{Alexiev2014-NKOS,
  author       = {Vladimir Alexiev and Jutta Lindenthal and Antoine Isaac},
  title        = {{On Compositionality of ISO 25964 Hierarchical Relations (BTG, BTP, BTI)}},
  booktitle    = {13th European Networked Knowledge Organization Systems (NKOS 2014)},
  year         = 2014,
  month        = sep,
  address      = {London, UK},
  url          = {http://vladimiralexiev.github.io/pres/20140912-NKOS-compositionality},
}

@Misc{Alexiev2014-UWash,
  author       = {Vladimir Alexiev},
  title        = {{Guest lecture (Module 9): Doing Business with Semantic Technologies}},
  howpublished = {INFX 598 - Introducing Linked Data: concepts, methods and tools. Information School, University of Washington},
  month        = may,
  year         = 2014,
  url          = {https://voicethread.com/myvoice/#thread/5784646/29625471/31274564},
  _url         = {http://www.slideshare.net/valexiev1/20140521-semtechbizguestlecture},
  abstract     = {Introduction to Ontotext and some of its products, clients and projects},
}

@TechReport{Alexiev2015-CH-names,
  author       = {Vladimir Alexiev},
  title        = {{Name Data Sources for Semantic Enrichment}},
  institution  = {Europeana Creative project},
  year         = 2015,
  type         = {{Part of Europeana Creative Deliverable D2.4}},
  month        = feb,
  url          = {http://vladimiralexiev.github.io/CH-names/README.html},
  abstract     = {Semantic enrichment in Europeana is a very difficult task due to several factors: 1. Varying metadata quality across different collections, sometimes including misallocation of metadata fields; 2. Varying metadata formatting practices across different collections, e.g. some collections indicate the role of a creator in brackets after the creator name; 3. Lack of accurate language information. In this report we focus on Person & Institution enrichment (person Named Entity Recognition), which in itself is an ambitious task. Historic people are often referred to by many names. For successful semantic enrichment it's important to integrate high-quality and high-coverage datasets that provide name info. There is a great number of Name Authority files maintained at libraries, museums and other heritage institutions world-wide, e.g. VIAF, ISNI, Getty ULAN, British Museum. Linked Open Data (LOD) datasets also have a plethora of names, e.g. in DBpedia, Wikidata and FreeBase. We analyze some of the available datasets in terms of person coverage, name coverage, language tags, extra features that can be useful for enrichment, quality. We also analyze the important topic of coreferencing, i.e. how connected the sources are to each other.},
}

@TechReport{Alexiev2015-EFD-classification,
  author       = {Vladimir Alexiev},
  title        = {{Europeana Food and Drink Classification Scheme}},
  institution  = {Europeana Food and Drink project},
  year         = 2015,
  type         = {Deliverable},
  number       = {D2.2},
  month        = feb,
  url          = {http://vladimiralexiev.github.io/pubs/Europeana-Food-and-Drink-Classification-Scheme-(D2.2).pdf},
  abstract     = {The Europeana Food and Drink Classification scheme (EFD classification) is a multi-dimensional scheme for discovering and classifying Cultural Heritage Objects (CHO) related to Food and Drink (FD). The topic of Food and Drink is so pervasive in our daily lives and in our culture that assembling a small "specialist" thesaurus is not feasible (such specialist thesauri were successfully used in other Europeana projects, eg ECLAP on performing arts and PartagePlus on Art Nouveau). We investigate about 20 datasets for their relevance to FD, including the Getty theasuri, Wordnet FD Domain, Wikipedia (in its 2 semantic data representations: DBpedia and Wikidata), AGROVOC, etc. We have selected Wikipedia as the basis for the classification, and plan to use the Wikipedia Categories to construct a hierarchical network to be used for classification.  The project will also use innovative semantic technologies to automate the extraction of terms and co-references. The result will be a body of semantically-enriched metadata that can support a wider range of multi-lingual applications such as search, discovery and browsing. (91 pages)},
}

@InProceedings{Alexiev2015-EFD-classification-pres,
  author       = {Vladimir Alexiev},
  title        = {{Europeana Food and Drink Classification Scheme}},
  booktitle    = {Europeana Food and Drink annual meeting},
  year         = 2015,
  month        = mar,
  address      = {Athens, Greece},
  url          = {http://www.slideshare.net/valexiev1/europeana-food-and-drink-classification-scheme},
}

@TechReport{Alexiev2015-EFD-semapp,
  author       = {Vladimir Alexiev},
  title        = {{Europeana Food and Drink Semantic Demonstrator Delivery}},
  institution  = {Europeana Food and Drink project},
  year         = 2015,
  type         = {Deliverable},
  number       = {D3.20},
  month        = oct,
  url          = {http://vladimiralexiev.github.io/pubs/Europeana-Food-and-Drink-Semantic-Demonstrator-Delivery-(D3.20).pdf},
}

@TechReport{Alexiev2015-EFD-semapp-progress1,
  author       = {Vladimir Alexiev},
  title        = {{Europeana Food and Drink Semantic Demonstrator M18 Progress Report}},
  institution  = {Europeana Food and Drink project},
  year         = 2015,
  type         = {Progress Report},
  number       = {D3.20a},
  month        = jun,
  url          = {http://vladimiralexiev.github.io/pubs/Europeana-Food-and-Drink-Semantic-Demonstrator-M18-Report-(D3.20a).pdf},
  abstract     = {Describes the development progress on the Europeana Food and Drink Semantic Demonstrator for the first 2.5 months (between 1 April 2015 and 15 June 2015), the achieved results, and project management considerations.}
}

@TechReport{Alexiev2015-EFD-semapp-spec,
  author       = {Vladimir Alexiev},
  title        = {{Europeana Food and Drink Semantic Demonstrator Specification}},
  institution  = {Europeana Food and Drink project},
  year         = 2015,
  type         = {Deliverable},
  number       = {D3.19},
  month        = mar,
  url          = {http://vladimiralexiev.github.io/pubs/Europeana-Food-and-Drink-Semantic-Demonstrator-Specification-(D3.19).pdf},
  abstract     = {The Europeana Food and Drink Semantic Demonstrator (EFD sem app) will allow multi-dimensional semantic exploration and discovery of cultural heritage objects (CHO) related to Food and Drink (FD). It will both apply and augment the EFD Classification scheme, using positive feedback loop mechanisms: the more the classification is used, the better it becomes. It will enable providers to classify their content, and consumers to explore CHOs using semantic search},
}

@InProceedings{Alexiev2015-GLAMs-Wikidata,
  author       = {Vladimir Alexiev},
  title        = {{GLAMs Working with Wikidata}},
  booktitle    = {Europeana Food and Drink content provider workshop},
  year         = 2015,
  month        = may,
  address      = {Athens, Greece},
  url          = {http://www.slideshare.net/valexiev1/glams-working-with-wikidata},
  abstract     = {How GLAMs can use Wikipedia/Wikidata to make their collections globally accessible across languages.},
}

@Article{Alexiev2015-IJDL,
  author       = {Vladimir Alexiev and Jutta Lindenthal and Antoine Isaac},
  title        = {{On the composition of ISO 25964 hierarchical relations (BTG, BTP, BTI)}},
  journal      = {International Journal on Digital Libraries},
  year         = 2015,
  pages        = {1-10},
  month        = aug,
  url          = {http://dx.doi.org/10.1007/s00799-015-0162-2},
  keywords     = {Thesauri; ISO 25964; BTG; BTP; BTI; Broader generic; Broader partitive; Broader instantial; AAT},
  issn         = {1432-5012},
  publisher    = {Springer Berlin Heidelberg},
  language     = {English},
  doi          = {10.1007/s00799-015-0162-2},
}

@InProceedings{Alexiev2015-bg.dbpedia,
  author       = {Vladimir Alexiev},
  title        = {{bg.dbpedia.org launched}},
  booktitle    = {DBpedia Meeting},
  year         = 2015,
  month        = feb,
  address      = {Dublin, Ireland},
  url          = {http://vladimiralexiev.github.io/pres/20150209-dbpedia/bg-dbpedia-launched.html},
  _url         = {http://vladimiralexiev.github.io/pres/20150209-dbpedia/bg-dbpedia-launched.pdf},
}

@InProceedings{Alexiev2015-dbpedia-mapping,
  author       = {Vladimir Alexiev},
  title        = {{Adding a DBpedia Mapping}},
  booktitle    = {DBpedia Meeting},
  year         = 2015,
  month        = feb,
  address      = {Dublin, Ireland},
  url          = {http://vladimiralexiev.github.io/pres/20150209-dbpedia/add-mapping.html},
  _url         = {http://vladimiralexiev.github.io/pres/20150209-dbpedia/add-mapping-long.html},
}

@InProceedings{Alexiev2015-dbpedia-problems,
  author       = {Vladimir Alexiev},
  title        = {{DBpedia Ontology and Mapping Problems}},
  booktitle    = {DBpedia Meeting},
  year         = 2015,
  month        = feb,
  address      = {Dublin, Ireland},
  url          = {http://vladimiralexiev.github.io/pres/20150209-dbpedia/dbpedia-problems.html},
  _url         = {http://vladimiralexiev.github.io/pres/20150209-dbpedia/dbpedia-problems-long.html},
}

@InProceedings{AlexievAngelova-CultJam15,
  author       = {Vladimir Alexiev and Dilyana Angelova},
  title        = {{O is for Open: OAI and SPARQL interfaces for Europeana}},
  booktitle    = {Europeana Creative Culture Jam},
  year         = 2015,
  month        = jul,
  address      = {Vienna, Austria},
  url          = {http://vladimiralexiev.github.io/pubs/O_is_for_Open_(CultJam_201507)_poster.pdf},
  _url         = {http://vladimiralexiev.github.io/pubs/O_is_for_Open_(CultJam_201507)_slide.pdf},
  abstract     = {Poster. As part of the Europeana Creative project, Ontotext added 2 additional channels to Europeana Labs: OAI & SPARQL, complementing the API. OAI is used for bulk donwload (e.g. to update the semantic repository). SPARQL can answer queries that the API cannot, e.g. linking objects, exploring contextual entities (e.g. parent places or author life dates), analytics/charts},
}

@InProceedings{AlexievAsenova2010-TeachingIT_PM,
  author       = {Vladimir Alexiev and Petya Asenova},
  title        = {{An Approach to Teaching IT Project Management in a Masters Program}},
  booktitle    = {6th Annual International Conference on Education in Computer Science},
  month        = jun,
  year         = 2010,
  address      = {Fulda and Munich, Germany},
  url          = {http://vladimiralexiev.github.io/pubs/AlexievAsenova2010-TeachingIT_PM.pdf},
}

@Book{AlexievBreuBruijn2005-InformationIntegration,
  author       = {Vladimir Alexiev and Michael Breu and Jos de Bruijn and Dieter Fensel and Ruben Lara and Holger Lausen},
  title        = {{Information Integration with Ontologies: Experiences from an Industrial Showcase}},
  chapter      = {2},
  publisher    = {John Wiley and Sons},
  month        = feb,
  year         = 2005,
  isbn         = {0-470-01048-7},
  abstract     = {Disparate information, spread over various sources, in various formats, and with inconsistent semantics is a major obstacle for enterprises to use this information at its full potential. Information Grids should allow for the effective access, extraction and linking of dispersed information. Currently Europe's coporations spend over 10 Billion EUR to deal with these problems. This book will demonstrate the applicability of grid technologies to industry. To this end, it gives a detailed insight on how on tology technology can be used to manage dispersed information assets more efficiently. The book is based on experiences from the COG (Corporate Ontology Grid) project, carried out jointly by three leading industrial players and the Digital Enterprise Research Institute Austria. Through comparisons of this project with alternative technologies and projects, it provides hands-on experience and best practice examples to act as a reference guide for their development. Information Integration with Ontologies: Ontology based Information Integration in an Industrial Setting is ideal for technical experts and computer researchers in the IT-area looking to achieve integration of heterogeneous information and apply ontology technologies and techniques in practice. It will also be of great benefit to technical decision makers seeking infor mation about ontology technologies and the scientific audience, interested in achievements towards the application of ontologies in an industrial setting.},
  url          = {http://books.google.com/books?isbn=0470010487},
}

@TechReport{AlexievCasamayor-FN-NIF,
  author       = {Vladimir Alexiev and Gerard Casamayor},
  title        = {{Integrating FrameNet in NIF}},
  institution  = {Ontotext Corp and Universitat Pompeu Fabra},
  year         = 2015,
  number       = {(draft)},
  month        = oct,
  url          = {http://vladimiralexiev.github.io/Multisensor/FrameNet/},
  keywords     = {FrameNet, NIF, NLP2RDF, RDF},
  abstract     = {FrameNet (FN) is a large-scale linguistic resource developed at Berkeley. It describes word senses and the situations they can play in ("valences") in terms of frames, frame elements and the links between them. FN has been converted to Linked Open Data (LOD) by ISTC-CNR, together with a large corpus of text annotated with FN. The NLP Interchange Format (NIF) represents linguistic data as RDF, faciliating interoperation of NLP tools and extensible data exchange. We review the FN-LOD representation and describe a possible way to integrate FN in NIF, proposed to be used in the MultiSensor project.},
}

@Article{AlexievMartev2009-ElectronicExportBG,
  author       = {Vladimir Alexiev and Teodor Martev},
  title        = {{Electronic Export Declarations Ease the Work of the Customs Agency and Traders}},
  journal      = {Computerworld (in Bulgarian)},
  year         = 2009,
  volume       = 46,
  month        = dec,
  annote       = {ECS2 was nominated for IT Project of the year 2010},
  url          = {http://vladimiralexiev.github.io/pubs/AlexievMartev2009-ElectronicExportBG.pdf},
}

@InProceedings{AlexievMitevBukev2010-eGovBPM,
  author       = {Vladimir Alexiev and Adrian Mitev and Alexander Bukev},
  title        = {{Implementing complex e-Government solutions with open source and BPM: Architecture of Export Control System phase 2 (ECS2)}},
  booktitle    = {Java2Days Conference},
  year         = 2009,
  address      = {Sofia, Bulgaria},
  url          = {http://vladimiralexiev.github.io/pubs/AlexievMitevBukev2010-eGovBPM.pdf},
}

@TechReport{AlexievTolosi2015-EFD-semapp-progress2,
  author       = {Vladimir Alexiev and Laura Tolosi},
  title        = {{Europeana Food and Drink Semantic Demonstrator M21 Progress Report}},
  institution  = {Europeana Food and Drink project},
  year         = 2015,
  type         = {Progress Report},
  number       = {D3.20b},
  month        = oct,
  url          = {http://vladimiralexiev.github.io/pubs/Europeana-Food-and-Drink-Semantic-Demonstrator-M21-Report-(D3.20b).pdf},
}

@Proceedings{CRMEX2013,
  title        = {{Practical Experiences with CIDOC CRM and its Extensions (CRMEX 2013), Workshop at 17th International Conference on Theory and Practice of Digital Libraries (TPDL 2013)}},
  booktitle    = {{Practical Experiences with CIDOC CRM and its Extensions (CRMEX 2013), Workshop at 17th International Conference on Theory and Practice of Digital Libraries (TPDL 2013)}},
  editor       = {Vladimir Alexiev and Vladimir Ivanov and Maurice Grinberg},
  abstract     = {The CIDOC CRM (international standard ISO 21127:2006) is a conceptual model and ontology with a fundamental role in many data integration efforts in the Digital Libraries and Cultural Heritage (CH) domain. The goal of this workshop is to describe and showcase systems using CRM at their core, exchange experience about the practical use of CRM, describe difficulties for the practical application of CRM, and share approaches for overcoming such difficulties. The ultimate objective of this workshop is to encourage the wider practical adoption of CRM},
  keywords     = {CIDOC CRM, RDF, Ontology, cultural heritage, practical applications},
  address      = {Valetta, Malta},
  month        = sep,
  year         = 2013,
  url          = {http://ceur-ws.org/Vol-1117/},
}

@TechReport{EDM-FRBRoo,
  author       = {Martin Doerr and Stefan Gradmann and others},
  title        = {{Europeana Task Force on EDM-FRBRoo Application Profile}},
  institution  = {Europeana},
  year         = 2013,
  month        = may,
  url          = {http://pro.europeana.eu/get-involved/europeana-tech/europeanatech-task-forces/edm-frbroo-application-profile},
  keywords     = {An EDM application profile that would allow a better representation of the FRBR group 1 entities: work, expression, manifestation and item. Contributed to Don Quixote mapping examples and the FRBRoo extension},
}

@TechReport{Europeana-enrichment-strategy,
  author       = {Juliane Stiller and Antoine Isaac and Vivien Petras and others},
  title        = {{EuropeanaTech Task Force on a Multilingual and Semantic Enrichment Strategy}},
  institution  = {Europeana},
  year         = 2014,
  type         = {Final report},
  month        = apr,
  url          = {http://pro.europeana.eu/get-involved/europeana-tech/europeanatech-task-forces/multilingual-and-semantic-enrichment-strategy},
  abstract     = {The semantic and multilingual enrichment of metadata in Europeana is a core concern as it improves access to the material, defines relations among objects and enables cross-lingual retrieval of documents. The quality of these enrichments is crucial to ensure that highly curated content from providers gets represented correctly across different languages. To ensure that those enrichments unfold their whole potential and act as facilitators of access, a semantic and multilingual enrichment strategy is needed. The EuropeanaTech Task Force on a Multilingual and Semantic Enrichment Strategy set out to analyze datasets in Europeana and to evaluate them with regard to their enrichment potential and the enrichments that were executed. The goal was to drive a strategy for enriching metadata fields adding value for users. To achieve this, the members of the task force held a one-day workshop in Berlin where they analyzed randomly selected datasets from Europeana, their metadata fields and their enrichment potential. This report aggregates the results and derives findings and recommendations regarding the metadata quality (source), vocabulary used (target) and the enrichment process. It was found that especially during mapping and ingestion time, metadata quality issues arise that influence the success of the enrichments. Tackling these issues with better documentation, training and the establishment of quality scores are some of the recommendations in this field. Furthermore, Europeana should encourage the delivery of specialized vocabularies with resolvable URIs which would also lead to less need for enrichments by Europeana itself. With regard to the enrichment process, clear rules for each field need to be established.},
}

@TechReport{Europeana-evaluation-enrichments,
  author       = {Antoine Isaac and Hugo Manguinhas and Juliane Stiller and Valentine Charles and others},
  title        = {{Report on Enrichment and Evaluation}},
  institution  = {Europeana Task Force on Enrichment and Evaluation},
  year         = 2015,
  type         = {Final report},
  month        = Oct,
  url          = {http://pro.europeana.eu/taskforce/evaluation-and-enrichments},
  abstract     = {This report on Evaluation and Enrichment provides an overview of the different processes in semantic enrichment and offers guidance on how to assess each of these steps to implement a coherent enrichment strategy. The report begins by introducing the terminology used in the report. While defining the notion of semantic enrichment, the Task Force has identified several other associated notions that are commonly used in the cultural heritage domain when addressing semantic enrichment. We also provide an overview of the enrichment tools and services developed in the Europeana Network over the past years, reflecting the diversity of processes at hand: tools for manual enrichment and annotation, tools for automatic enrichment and workflow design tools. We also focus on the interoperability issues such as rules for specifying the linking or the format used to describe the enrichment outputs. As well as looking at the details of the enrichment processes we pick up the work done by the previous Task Force by specifying criteria for selecting and assessing target datasets. These criteria are based on vocabularies and datasets examples relevant to the Cultural Heritage domain. This selection strategy is available in a companion document to this report. The last component of the enrichment strategy is the evaluation of the enrichment processes. So far, evaluation in this domain has not been much documented even though a lot of work has been done in the field. We have tried to summarise different evaluation methodologies developed in related projects. These methods highlight the different components of the enrichment process that can be subject to evaluation. In order to validate all the recommendations provided in the previous sections, we have performed a quantitative and qualitative evaluation of seven enrichment services on a same subset of the Europeana dataset. The report of the evaluation is available in a companion document to this report while the main conclusions remain in this report. This report is a result of an inventory of tools, practices and standards that define the current state of the art for semantic enrichment. The analysis and evaluation work done during the course of the Task Force have allowed us to compile a series of lessons learnt that should be considered for the design and enhancement of enrichment services and their evaluation},
}

@Manual{GVP-LOD-doc,
  title        = {{Getty Vocabularies Linked Open Data: Semantic Representation}},
  author       = {Vladimir Alexiev and Joan Cobb and Gregg Garcia and Patricia Harpring},
  organization = {Getty Research Institute},
  edition      = {3.0},
  month        = mar,
  year         = 2015,
  url          = {http://vocab.getty.edu/doc/},
}

@Manual{GVP-LOD-queries,
  title        = {{Getty Vocabularies: LOD Sample Queries}},
  author       = {Vladimir Alexiev},
  organization = {Getty Research Institute},
  edition      = {3.0},
  month        = mar,
  year         = 2015,
  url          = {http://vocab.getty.edu/doc/queries/},
  abstract     = {We provide 90 sample queries for the Getty Vocabularies LOD that should allow you to learn to query the data effectively. We include searching for data, getting all data of a subject, all labels and their attributes, full-text search, getting an ordered hierarchy, charts, etc. The queries are organized in sections: general, TGN-specific, ULAN-specific, Language queries, Counting and descriptive info, Exploring the ontology},
}

@TechReport{GVP-ontology,
  author       = {Vladimir Alexiev},
  title        = {{Getty Vocabulary Program (GVP) Ontology 3.0}},
  institution  = {Getty Research Institute},
  type         = {Namespace Document},
  year         = 2015,
  month        = mar,
  url          = {http://vocab.getty.edu/ontology},
  edition      = {3.0},
  abstract     = {The GVP Ontology defines classes, properties and values (skos:Concepts) used in GVP LOD. As of version 3.0, it is complete regarding AAT, TGN and ULAN, and will be extended in time with more elements needed for other GVP vocabularies (CONA). It uses the SKOS, SKOS-XL, ISO 25964; DC, DCT, BIBO, FOAF, BIO, Schema, PROV, WGS84 ontologies. Description at http://lov.okfn.org/dataset/lov/details/vocabulary_gvp.html},
}

@InProceedings{Alexiev2015-Glam-Wiki,
  author       = {Vladimir Alexiev and Valentine Charles and Hugo Manguinhas},
  title        = {{Wikidata, a Target for Europeana's Semantic Strategy}},
  booktitle    = {Glam-Wiki 2015},
  year         = 2015,
  month        = apr,
  address      = {The Hague},
  url          = {http://www.slideshare.net/valexiev1/wikidata-a-target-for-europeanas-semantic-strategy-glamwiki-2015},
  _url         = {http://vladimiralexiev.github.io/pubs/GLAMwiki2015.ppt},
  _url         = {https://nl.wikimedia.org/wiki/GLAM-WIKI_2015/Programme/Discussions/Strategy#Presentation:_Wikidata.2C_a_target_for_Europeana.E2.80.99s_semantic_strategy.3F},
  abstract     = {For Europeana, the platform for Europe’s digital cultural heritage from libraries, museums and archives, getting richer (semantic and multilingual) metadata is a priority. It improves access to the 40 million cultural heritage objects, notably enabling the multilingual retrieval of documents and creates relations between objects. To enhance data and enable retrieval across languages, Europeana performs automatic enrichment by selecting source metadata field(s) in the Europeana data and creating links to a selected target vocabulary or dataset representing contextual resources such as places, concepts, agents and time periods. Wikidata is since a while on Europeana’s radar as a potential new target for enrichment but how can it be integrated with cultural heritage data?},
}

@Article{HCLS-paper,
  author       = {Michel Dumontier and Alasdair J. G. Gray and M. Scott Marshall and Vladimir Alexiev and others},
  title        = {{The Health Care and Life Sciences Community Profile for Dataset Descriptions}},
  journal      = {PeerJ},
  year         = 2015,
  volume       = {submitted},
  month        = nov,
  abstract     = {Access to consistent, high-quality metadata is critical to finding, understanding, and reusing scientific data. However, while there are many relevant vocabularies for the annotation of a dataset, none sufficiently captures all the necessary metadata. This prevents uniform indexing and querying of dataset repositories. Towards providing a practical guide for producing a high quality description of biomedical datasets, the W3C Semantic Web for Health Care and the Life Sciences Interest Group (HCLSIG) identified RDF vocabularies that could be used to specify common metadata elements and their value sets. The resulting guideline covers elements of description, identification, attribution, versioning, provenance, and content summarization. This guideline reuses existing vocabularies, and is intended to meet key functional requirements including indexing, discovery, exchange, query, and retrieval of datasets. The resulting metadata profile is generic and could be used by other domains with an interest in providing machine readable descriptions of versioned datasets.},
}

@TechReport{HCLS-profile,
  author       = {Alasdair J. G. Gray and Joachim Baran abd M. Scott Marshall and Michel Dumontier and Vladimir Alexiev and others},
  title        = {{Dataset Descriptions: HCLS Community Profile}},
  institution  = {Semantic Web in Health Care and Life Sciences Interest Group (HCLSIG)},
  year         = 2015,
  month        = may,
  url          = {https://github.com/W3C-HCLSIG/HCLSDatasetDescriptions},
  abstract     = {Access to consistent, high-quality metadata is critical to finding, understanding, and reusing scientific data. This document describes a consensus among participating stakeholders in the Health Care and the Life Sciences domain on the description of datasets using the Resource Description Framework (RDF). This specification meets key functional requirements, reuses existing vocabularies to the extent that it is possible, and addresses elements of data description, versioning, provenance, discovery, exchange, query, and retrieval.},
}

@Manual{ISO-25964-owl,
  title        = {{ISO 25964 Part 1: Thesauri for information retrieval: RDF/OWL vocabulary, extension of SKOS and SKOS-XL}},
  author       = {Johan De Smedt and Antoine Isaac and Stella Dextre Clarke and Jutta Lindenthal and Marcia Lei Zeng and Douglas S. Tudhope and Leonard Will and Vladimir Alexiev},
  month        = dec,
  year         = 2013,
  abstract     = {OWL ontology representing the newest ISO standard on thesauri. Description at http://lov.okfn.org/dataset/lov/details/vocabulary_iso-thes.html},
  url          = {http://purl.org/iso25964/skos-thes},
}

@InProceedings{Ikonomov2013-EuropeanaCreative-EDM,
  author       = {Nikola Ikonomov and Boyan Simeonov and Jana Parvanova and Vladimir Alexiev},
  title        = {{Europeana Creative. EDM Endpoint. Custom Views}},
  booktitle    = {Digital Presentation and Preservation of Cultural and Scientific Heritage (DiPP 2013)},
  year         = 2013,
  month        = sep,
  address      = {Veliko Tarnovo, Bulgaria},
  url          = {http://vladimiralexiev.github.io/pubs/Ikonomov2013-EuropeanaCreative-EDM.pdf},
  keywords     = {cultural heritage, Europeana, EDM, ESE, Semantic Web, RDF, SKOS, URI, OWLIM, Semantic Repository, SPARQL, Query, Endpoint},
  abstract     = {The paper discusses the Europeana Creative project which aims to facilitate re-use of cultural heritage metadata and content by the creative industries. The paper focuses on the contribution of Ontotext to the project activities. The Europeana Data Model (EDM) is further discussed as a new proposal for structuring the data that Europeana will ingest, manage and publish. The advan-tages of using EDM instead of the current ESE metadata set are highlighted. Finally, Ontotext's EDM Endpoint is presented, based on OWLIM semantic re-pository and SPARQL query language. A user-friendly RDF view is presented in order to illustrate the possibilities of Forest - an extensible modular user interface framework for creating linked data and semantic web applications.},
  _url         = {http://vladimiralexiev.github.io/pubs/Ikonomov2013-EuropeanaCreative-EDM-slides.pdf},
  _url         = {http://www.slideshare.net/valexiev1/ikonomov2013-europeana-creativeedmslides},
}

@TechReport{LDBC-D442,
  author       = {Vassilis Papakonstantinou and Irini Fundulaki and Giorgos Flouris and Vladimir Alexiev},
  title        = {{Benchmark Design for Reasoning}},
  institution  = {Linked Data Benchmarking Council project},
  year         = 2014,
  type         = {{Deliverable D4.4.2}},
  month        = sep,
  url          = {http://ldbcouncil.org/sites/default/files/LDBC_D4.4.2.pdf},
  abstract     = {Reasoning (mainly OWL reasoning) has received increasing attention by ontology designers for more accurately representing the domain at hand. To reflect this importance, one of LDBC’s objectives is to identify a set of interesting use cases that consider OWL reasoning constructs (beyond the usual RDFS constructs) that can be used to challenge existing RDF engines or repositories. This Deliverable has two parts: in the first part, we present four different sets of queries that can be used to determine whether RDF query engines take into account OWL constructs during query plan construction or query execution; in the second part we consider how a repository or query engine incorporates and considers business rules, i.e., domain-specific rules that follow common templates, useful in practical applications.},
}

@InProceedings{MarinovAlexievDjonev1994-BCPN,
  author       = {Georgi Marinov and Vladimir Alexiev and Yavor Djonev},
  editor       = {P. Jorrand and V. Sgurev},
  booktitle    = {Artifical Intelligence: Methodology, Systems, and Applications (AIMSA'94)},
  title        = {{Boolean Constraint Propagation Networks}},
  publisher    = {World Scientific Publishing},
  address      = {Sofia, Bulgaria},
  month        = sep,
  year         = 1994,
  pages        = {109-118},
  abstract     = {An overview of existing applications of Linear Logic (LL) to issues of computation. After a substantial introduction to LL, it discusses the implications of LL to functional programming, logic programming, concurrent and object-oriented programming and some other applications of LL, like semantics of negation in LP, non-monotonic issues in AI planning, \etc. Although the overview covers pretty much the state-of-the-art in this area, by necessity many of the works are only mentioned and referenced, but not discussed in any considerable detail. The paper does not presuppose any previous exposition to LL, and is addressed more to computer scientists (probably with a theoretical inclination) than to logicians. The paper contains over 140 references, of which some 80 are about applications of LL.},
  url          = {http://vladimiralexiev.github.io/pubs/MarinovAlexievDjonev1994-BCPN.pdf},
}

@TechReport{OldmanMahmudAlexiev2013,
  author       = {Dominic Oldman and Joshan Mahmud and Vladimir Alexiev},
  title        = {{The Conceptual Reference Model Revealed. Quality contextual data for research and engagement: A British Museum case study}},
  institution  = {ResearchSpace Project},
  year         = 2013,
  type         = {Draft 0.98},
  month        = jul,
  url          = {http://confluence.ontotext.com/display/ResearchSpace/BM+Mapping},
  pages        = {359 pages},
  abstract     = {Contents: 169p: Main body, including discussion, illustrations and mapping diagrams. 7p: Association Codes (see details at BM Association Mapping v2). 49p: Example Object Graph. 134p: RDFer configuration files (i.e. mapping implementation)},
}

@InProceedings{Parvanova2013-SemanticAnnotation,
  author       = {Jana Parvanova and Vladimir Alexiev and Stanislav Kostadinov},
  title        = {{RDF Data and Image Annotations in ResearchSpace}},
  booktitle    = {Collaborative Annotations in Shared Environments: metadata, vocabularies and techniques in the Digital Humanities ({DH-CASE} 2013). Collocated with {DocEng} 2013},
  abstract     = {This paper presents the approaches to data and image annotation in ResearchSpace (http://www.researchspace.org), an Andrew W. Mellon Foundation funded project led by the British Museum aimed at supporting collaborative internet research, information sharing and web applications for the cultural heritage scholarly community},
  keywords     = {Computer-supported collaborative work, Annotation, Museum applications, Cultural heritage online},
  address      = {Florence, Italy},
  month        = sep,
  year         = 2013,
  url          = {http://vladimiralexiev.github.io/pubs/Parvanova2013-SemanticAnnotation.pdf},
  _url         = {http://vladimiralexiev.github.io/pubs/Parvanova2013-SemanticAnnotation-slides.pdf},
}

@InProceedings{TagarevTolosiAlexiev-FD,
  author       = {Andrey Tagarev and Laura Tolosi and Vladimir Alexiev},
  title        = {{Domain-specific modeling: Towards a Food and Drink Gazetteer}},
  booktitle    = {First International Keystone Conference},
  year         = 2015,
  month        = sep,
  address      = {Coimbra, Portugal},
  url          = {http://vladimiralexiev.github.io/pubs/Tagarev2015-DomainSpecificGazetteer.pdf},
  keywords     = {categorization, Wikipedia, Wikipedia categories, gazetteer, Europeana, Cultural Heritage, concept extraction},
  _url         = {http://vladimiralexiev.github.io/pubs/Tagarev2015-DomainSpecificGazetteer-slides.pdf},
  abstract     = {Our goal is to build a Food and Drink (FD) gazetteer that can serve for classification of general, FD-related concepts, efficient faceted search or automated semantic enrichment. Fully supervised design of a domain-specific models "ex novo" is not scalable. Integration of several ready knowledge bases is tedious and does not ensure coverage. Completely data-driven approaches require a large amount of training data, which is not always available. In cases when the domain is not very specific (as the FD domain), re-using encyclopedic knowledge bases like Wikipedia may be a good idea. We propose here a semi-supervised approach, that uses a restricted Wikipedia as a base for the modeling, achieved by selecting a domain-relevant Wikipedia category as root for the model and all its subcategories, combined with expert and data-driven pruning of irrelevant categories.},
}
