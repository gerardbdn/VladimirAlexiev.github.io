-*- coding: utf-8; fill-column: 5000 -*-

@TechReport{Alexiev1993-annotatedBibliography,
  author       = {Vladimir Alexiev},
  title        = {{A (Not Very Much) Annotated Bibliography on Integrating Object-Oriented and Logic Programming}},
  institution  = {University of Alberta},
  year         = 1993,
  month        = mar,
  url          = {http://vladimiralexiev.github.io/pubs/Alexiev1993-annotatedBibliography.pdf},
  url_CiteSeer = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.54.6168},
  keywords     = {object-oriented programming, logic programming, multiparadigm programming, bibliography},
  abstract     = {An overview of existing applications of Linear Logic (LL) to issues of computation. After a substantial introduction to LL, it discusses the implications of LL to functional programming, logic programming, concurrent and object-oriented programming and some other applications of LL, like semantics of negation in LP, non-monotonic issues in AI planning, etc. Although the overview covers pretty much the state-of-the-art in this area, by necessity many of the works are only mentioned and referenced, but not discussed in any considerable detail. The paper does not presuppose any previous exposition to LL, and is addressed more to computer scientists (probably with a theoretical inclination) than to logicians. The paper contains over 140 references, of which some 80 are about applications of LL.},
}

@TechReport{Alexiev1993-mutableObjectState,
  author       = {Vladimir Alexiev},
  title        = {{Mutable Object State for Object-Oriented Logic Programming: A Survey}},
  institution  = {University of Alberta},
  year         = 1993,
  number       = {TR93-15},
  month        = aug,
  url          = {http://vladimiralexiev.github.io/pubs/Alexiev1993-mutableObjectState.pdf},
  keywords     = {object-oriented programming, logic programming, multiparadigm programming, mutable, object state, survey},
  abstract     = {One of the most difficult problems on the way to an integration of Object-Oriented and Logic Programming is the modeling of changeable object state(i.e. object dynamics) in a particular logic in order not to forfeit the declarative nature of LP. Classical logic is largely unsuitable for such a task, because it adopts a general (both temporally and spatially), Platonic notion of validity, whereas object state changes over time and is local to an object. This paper presents the problem and surveys the state-of-the-art approaches to its solution, as well as some emerging, promising new approaches. The paper tries to relate the different approaches, to evaluate their merits and deficiencies and to identify promising directions for development. The emphasis in this survey is on efficient implementation of state change, one which would be suitable for the lowest fundamental level of a general OOLP language. The following approaches are covered: Assert/Retract, Declarative Database Updates and Transaction Logic, Modal and Dynamic Logics, Perpetual Objects, Logical Objects and Linear Objects, Linear Logic, Rewriting Logic and MaudeLog.},
}

@TechReport{Alexiev1993-objectOriented,
  author       = {Vladimir Alexiev},
  title        = {{Object-Oriented and Logic-Based Knowledge Representation}},
  institution  = {University of Alberta},
  year         = 1993,
  url          = {http://vladimiralexiev.github.io/pubs/Alexiev1993-objectOriented.pdf},
  url_CiteSeer = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.54.2657},
  keywords     = {object-oriented programming, logic programming, multiparadigm programming, knowledge representation},
  howpublished = {Term project},
  note         = {Term project},
  abstract     = {This paper is a survey of a number of languages/systems based on both Object-Oriented and Logic Programming and designed expressly for Knowledge Representation tasks. My goal in the paper is to argue that the integration of these two paradigms (particularly the synergism that emerges from such an integration_ forms a stable basis for Knowledge Representation at the symbolic level. I try to support this claim both by examples from the papers surveyed and by considerations in a more general context. Some more advanced topics concerning special-purpose non-classic logics are also discussed.},
}

@Article{Alexiev1994-applicationsLinearLogic,
  author       = {Vladimir Alexiev},
  title        = {{Applications of Linear Logic to Computation: An Overview}},
  journal      = {Bulletin of the IGPL},
  year         = 1994,
  volume       = 2,
  number       = 1,
  pages        = {77-107},
  month        = mar,
  url          = {http://vladimiralexiev.github.io/pubs/Alexiev1994-applicationsLinearLogic.pdf},
  url_Published= {http://jigpal.oxfordjournals.org/content/2/1/77},
  keywords     = {linear logic, survey},
  issn         = {1368-9894},
  note         = {Also University of Alberta TR93-18, December 1993},
  doi          = {10.1093/jigpal/2.1.77},
}

@TechReport{Alexiev1995-eventCalculus,
  author       = {Vladimir Alexiev},
  title        = {{The Event Calculus as a Linear Logic Program}},
  institution  = {University of Alberta},
  year         = 1995,
  number       = {TR95-24},
  month        = sep,
  url          = {http://vladimiralexiev.github.io/pubs/Alexiev1995-eventCalculus.pdf},
  url_CiteSeer = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.30.9953},
  url_TR       = {https://era.library.ualberta.ca/downloads/9g54xk065},
  keywords     = {event calculus, linear logic, negation as failure, knowledge update},
  abstract     = {The traditional presentation of Kowalski's Event Calculus as a logic program uses Negation- as-Failure (NAF) in an essential way to support persistence of fluents. In this paper we present an implementation of Event Calculus as a purely logical (without NAF) Linear Logic (LL) program. This work demonstrates some of the internal non-monotonic features of LL and its suitability for knowledge update (as opposed to knowledge revision). Although NAF is an ontologically sufficient solution to the frame problem, the LL solution is implementationally superior. Handling of incomplete temporal descriptions and support for ramifications (derived fluents) are also considered.},
  keywords     = {event calculus, linear logic},
}

@Misc{Alexiev1995-thesisProposal,
  author       = {Vladimir Alexiev},
  title        = {{Object-Oriented Logic Programming based on Linear Logic}},
  month        = feb,
  year         = 1995,
  keywords     = {object-oriented programming, logic programming, multiparadigm programming, linear logic},
  institution  = {University of Alberta},
  howpublised  = {Thesis proposal},
  note         = {Thesis proposal},
}

@InProceedings{Alexiev1996-targetedCommunication,
  author       = {Vladimir Alexiev},
  title        = {{Targeted Communication in Linear Objects}},
  booktitle    = {Artificial Intelligence: Methodology, Systems, Applications (AIMSA'96)},
  year         = 1996,
  month        = sep,
  publisher    = {IOI Press},
  url          = {http://vladimiralexiev.github.io/pubs/Alexiev1996-targetedCommunication.pdf},
  url_CiteSeer = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.47.7107},
  url_TR       = {https://era.library.ualberta.ca/downloads/bc386k59r},
  keywords     = {Linear Objects, communication, broadcasting, object-oriented programming, logic programming, linear logic},
  note         = {Also University of Alberta TR94-14},
  abstract     = {Linear Objects (LO) of Andreoli and Pareschi is the first proposal to integrate object-oriented programming into logic programming based on Girard's Linear Logic (LL). In LO each object is represented by a separate open node of a proof tree. This ``insulates'' objects from one another which allows the attributes of an object to be represented as a multiset of atoms and thus facilitates easy retrieval and update of attributes. However this separation hinders communication between objects. Communication in LO is achieved through broadcasting to all objects which in our opinion is infeasible from a computational viewpoint. This paper proposes a refined communication mechanism for LO which uses explicit communication channels specified by the programmer. We name it TCLO which stands for ``Targeted Communication in LO''. Although channel specification puts some burden on the programmer, we demonstrate that the language is expressive enough by redoing some of the examples given for LO. Broadcasting can be done in a controlled manner. LO can be seen as a special case of TCLO where only one global channel (the forum) is used.},
}

@TechReport{Alexiev1998-distributedSynchronization,
  author       = {Vladimir Alexiev},
  title        = {{Distributed Synchronization in a pi-Calculus with Bidirectional Communication}},
  institution  = {University of Alberta},
  year         = 1998,
  month        = jan,
  url          = {http://vladimiralexiev.github.io/pubs/Alexiev1998-distributedSynchronization.pdf},
  url_CiteSeer = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.301.8273},
  keywords     = {pi-calculus, input prefix, distributed synchronization, communication},
  abstract     = {The (input) prefix operation of the pi-calculus expresses global synchronization (blocking) of the prefixed process. We show how to implement synchronization in a completely distributed manner, by using bidirectional atomic communication and the principle of provision (data-dependency-based synchronization)},
}

@TechReport{Alexiev1998-finitePi,
  author       = {Vladimir Alexiev},
  title        = {{Representing the Finite pi-calculus in Multi-Interaction Nets: Concurrency = Interaction + Non-determinism}},
  institution  = {University of Alberta},
  year         = 1998,
  month        = apr,
  url          = {http://vladimiralexiev.github.io/pubs/Alexiev1998-finitePi.pdf},
  url_CiteSeer = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.301.8381},
  keywords     = {pi-calculus, interaction nets, linear logic, concurrent computation,distributed computation},
  abstract     = {We extend the Interaction Nets of Lafont with some non-determinism capabilities and then show how to implement the finite monadic pi-calculus in that system},
}

@PhdThesis{Alexiev1999-thesis,
  author       = {Vladimir Alexiev},
  title        = {{Non-deterministic Interaction Nets}},
  school       = {University of Alberta},
  year         = 1999,
  url          = {http://vladimiralexiev.github.io/pubs/Alexiev1999-thesis.pdf},
  url_Slides   = {http://vladimiralexiev.github.io/pubs/Alexiev1999-thesisPresentation.pdf},
  url_Other    = {http://vladimiralexiev.github.io/pubs/Alexiev1999-thesis-2up.pdf},
  abstract     = {The Interaction Nets (IN) of Lafont are a graphical formalism used to model parallel computation. Their genesis can be traced back to the Proof Nets of Linear Logic. They enjoy several nice theoretical properties, amongst them pure locality of interaction, strong confluence, computational completeness, syntactically-definable deadlock-free fragments, combinatorial completeness (existence of a Universal IN). They also have nice "pragmatic" properties: they are simple and elegant, intuitive, can capture aspects of computation at widely varying levels of abstraction. Compared to term and graph rewriting systems, INs are much simpler (a subset of such systems that imposes several constraints on the rewriting process), but are still computationally complete (can capture the lambda-calculus). INs are a refinement of graph rewriting which keeps only the essential features in the system.
Conventional INs are strongly confluent, and are therefore unsuitable for the modeling of non-deterministic systems such as process calculi and concurrent object-oriented programming. We study four diffrent ways of "breaking" the confluence of INs by introducing various extensions:
- IN with Multiple (reduction) Rules (INMR); Allow more than one reduction rule per redex.
- IN with Multiple Principal Ports (INMPP): Allow more than one active port per node.
- IN with MultiPorts (INMP): Allow more than one connection per port.
- IN with Multiple Connections (INMC): Allow hyper-edges (in the graph-theoretical sense), i.e. connections between more than two ports.

We study in considerable detail the relative expressive power of these systems, both by representing various programming examples in them, and by constructing inter-representations that translate nets from one system to another.
We study formally a translation from the finite pi-calculus to a system that we call MultiInteraction Nets: MIN=INMP+NMPP. We prove the faithfulness of the translation to the pi-calculus processes that it represents, both structural and operational (completeness and soundness of reduction). We show that unlike the pi-calculus, our translation implements the Prefix operation of the pi-calculus in a distributed and purely local manner, and implements explicitly the distribution and duplication of values to the corresponding occurrences of a variable. 
We compare our translation to other graphical and combinatory representations of the pi-calculus, such as the pi-nets of Milner, the Interaction Diagrams of Parrow, and the Concurrent Combinators of Honda and Yoshida. 
The original paper on IN (Lafont, 1990) states that INs were designed to be simple and practical; to be a "programming language that can be used for the design of interactive software". However, to date INs have been used only for theoretical investigations. This thesis is mostly devoted to a hands-on exploration of applications of IN to various "programming problems".}
}

@TechReport{Alexiev2004-DataIntegration,
  author       = {Vladimir Alexiev},
  title        = {{Data Integration Survey}},
  institution  = {European project "Corporate Ontology Grid" (COG)},
  type         = {Deliverable},
  month        = sep,
  year         = 2004,
}

@Misc{Alexiev2010-costEffectiveEGov,
  author       = {Vladimir Alexiev},
  title        = {{Cost-effective e-Government Services: Export Control System phase 2 (ECS2)}},
  month        = feb,
  year         = 2010,
  url_Slides   = {http://vladimiralexiev.github.io/pubs/Alexiev2010-costEffectiveEGov.pdf},
  keywords     = {model-driven development, e-customs, e-government, Export Control System},
  type         = {presentation},
  booktitle    = {Bulgaria-Korea IT Experts Workshop},
  address      = {Sofia, Bulgaria},
}

@Misc{Alexiev2011-KIM-Stanbol,
  author       = {Vladimir Alexiev},
  title        = {{Comparing Ontotext KIM and Apache Stanbol}},
  month        = sep,
  year         = 2011,
  url_Slides   = {http://www.slideshare.net/valexiev1/comparing-ontotext-kim-and-apache-stanbol},
  url_Other    = {http://www.slideshare.net/valexiev1/comparing-ontotext-kim-and-apache-stanbol-appendix},
  keywords     = {semantic enrichment, text analysis, Ontotext KIM, Apache Stanbol},
  type         = {presentation},
  institution  = {Ontotext Corp},
}

@Misc{Alexiev2011-SemtechForCulturalHeritage,
  author       = {Vladimir Alexiev},
  title        = {{Semantic Technologies for Cultural Heritage}},
  month        = may,
  year         = 2011,
  url_Slides   = {http://vladimiralexiev.github.io/pubs/Alexiev2011-SemtechForCulturalHeritage.pdf},
  keywords     = {semantic technology, ontology, semantic integration, cultural heritage},
  type         = {presentation},
  booktitle    = {Global Smart SOC Initiative Summit},
  address      = {Seoul, Korea},
}

@InProceedings{Alexiev2012-CRM-properties,
  author       = {Vladimir Alexiev},
  title        = {{Types and Annotations for CIDOC CRM Properties}},
  booktitle    = {Digital Presentation and Preservation of Cultural and Scientific Heritage (DiPP2012) conference (Invited report)},
  year         = 2012,
  month        = sep,
  address      = {Veliko Tarnovo, Bulgaria},
  url          = {http://vladimiralexiev.github.io/pubs/Alexiev2012-CRM-Properties.pdf},
  url_Slides   = {http://vladimiralexiev.github.io/pubs/Alexiev2012-CRM-Properties-presentation.ppt},
  keywords     = {cultural heritage, semantic technology, ontology, CIDOC CRM, properties, attribute assignment, reification, property reification},
  abstract     = {The CIDOC CRM provides an extensive ontology for describing entities and properties appearing in cultural heritage (CH) documentation, history and archeology. CRM provides some means for describing information about properties (property types, attribute assignment, and "long-cuts") and guidelines for extending the vocabulary. However, these means are far from complete, and in some cases there is little guidance how to "implement" them in RDF. In this article we outline the prob-lems, relate them to established RDF patterns and mechanisms, and describe several implementation alternatives.},
}

@InProceedings{Alexiev2012-CRM-search,
  author       = {Vladimir Alexiev},
  title        = {{Implementing CIDOC CRM Search Based on Fundamental Relations and OWLIM Rules}},
  booktitle    = {Workshop on Semantic Digital Archives (SDA 2012), part of International Conference on Theory and Practice of Digital Libraries (TPDL 2012)},
  year         = 2012,
  volume       = 912,
  month        = sep,
  address      = {Paphos, Cyprus},
  publisher    = {CEUR WS},
  url          = {http://vladimiralexiev.github.io/pubs/Alexiev2012-CRM-FR-search.pdf},
  url_Slides   = {http://vladimiralexiev.github.io/pubs/Alexiev2012-CRM-Search-presentation.pdf},
  url_Published= {http://ceur-ws.org/Vol-912/paper8.pdf},
  keywords     = {cultural heritage, semantic technology, ontology, CIDOC CRM, semantic search, Fundamental Concepts, Fundamental Relations},
  url_Slides   = {http://sda2012.dke-research.de/images/pdfs/crm-search-presentation.pdf},
  abstract     = {The CIDOC CRM provides an ontology for describing entities, properties and relationships appearing in cultural heritage (CH) documentation, history and archeology. CRM promotes shared understanding by providing an extensible semantic framework that any CH information can be mapped to. CRM data is usually represented in semantic web format (RDF) and comprises complex graphs of nodes and properties. An important question is how a user can search through such complex graphs, since the number of possible combinations is staggering. One approach "com-presses" the semantic network by mapping many CRM entity classes to a few "Fundamental Concepts" (FC), and mapping whole networks of CRM proper-ties to fewer "Fundamental Relations" (FR). These FC and FRs serve as a "search index" over the CRM semantic web and allow the user to use a simpler query vocabulary. We describe an implementation of CRM FR Search based on OWLIM Rules, done as part of the ResearchSpace (RS) project. We describe the technical de-tails, problems and difficulties encountered, benefits and disadvantages of using OWLIM rules, and preliminary performance results. We provide implementation experience that can be valuable for further implementation, definition and maintenance of CRM FRs.},
}

@Proceedings{CRMEX2013,
  title        = {{Practical Experiences with CIDOC CRM and its Extensions (CRMEX 2013), Workshop at 17th International Conference on Theory and Practice of Digital Libraries (TPDL 2013)}},
  year         = 2013,
  booktitle    = {{Practical Experiences with CIDOC CRM and its Extensions (CRMEX 2013), Workshop at 17th International Conference on Theory and Practice of Digital Libraries (TPDL 2013)}},
  editor       = {Vladimir Alexiev and Vladimir Ivanov and Maurice Grinberg},
  volume       = 1117,
  address      = {Valetta, Malta},
  month        = sep,
  publisher    = {CEUR WS},
  url          = {http://ceur-ws.org/Vol-1117/},
  keywords     = {CIDOC CRM, RDF, Ontology, cultural heritage, practical applications},
  abstract     = {The CIDOC CRM (international standard ISO 21127:2006) is a conceptual model and ontology with a fundamental role in many data integration efforts in the Digital Libraries and Cultural Heritage (CH) domain. The goal of this workshop is to describe and showcase systems using CRM at their core, exchange experience about the practical use of CRM, describe difficulties for the practical application of CRM, and share approaches for overcoming such difficulties. The ultimate objective of this workshop is to encourage the wider practical adoption of CRM},
}

@InProceedings{Alexiev2013-CRM-reasoning,
  author       = {Vladimir Alexiev and Dimitar Manov and Jana Parvanova and Svetoslav Petrov},
  title        = {{Large-scale Reasoning with a Complex Cultural Heritage Ontology (CIDOC CRM)}},
  booktitle    = {Workshop Practical Experiences with CIDOC CRM and its Extensions (CRMEX 2013) at TPDL 2013},
  year         = 2013,
  volume       = 1117,
  month        = sep,
  address      = {Valetta, Malta},
  publisher    = {CEUR WS},
  url          = {http://vladimiralexiev.github.io/pubs/Alexiev2013-CRM-reasoning.pdf},
  url_Slides   = {http://vladimiralexiev.github.io/pubs/Alexiev2013-CRM-reasoning-slides.ppt},
  url_Published= {http://ceur-ws.org/Vol-1117/paper8.pdf},
  keywords     = {cultural heritage, semantic technology, ontology, CIDOC CRM, semantic search, Fundamental Relations, OWLIM, semantic repository, inference, performance, Ontotext GraphDB},
  abstract     = {The CIDOC Conceptual Reference Model (CRM) is an important ontology in the Cultural Heritage (CH) domain. CRM is intended mostly as a data integration mechanism, allowing reasoning and discoverability across diverse CH sources represented in CRM. CRM data comprises complex graphs of nodes and properties. An important question is how to search through such complex graphs, since the number of possible combinations is staggering. One answer is the "Fundamental Relations" (FR) approach that maps whole networks of CRM properties to fewer FRs, serving as a "search index" over the CRM semantic web. We present performance results for an FR Search implementation based on OWLIM. This search works over a significant CH dataset: almost 1B statements resulting from 2M objects of the British Museum. This is an exciting demonstration of large-scale reasoning with real-world data over a complex ontology (CIDOC CRM). We present volumetrics, hardware specs, compare the numbers to other repositories hosted by Ontotext, performance results, and compare performance of a SPARQL implementation.},
}

@Misc{Alexiev2013-ResearchSpace,
  type         = {presentation},
  author       = {Vladimir Alexiev},
  title        = {{ResearchSpace as an Example of a VRE Based on CIDOC CRM}},
  booktitle    = {Virtual Center for Medieval Studies (Medioevo Europeo VCMS) Workshop},
  year         = 2013,
  month        = apr,
  address      = {Bucharest, Romania},
  url_Slides   = {http://www.slideshare.net/valexiev1/research-space-vre-based-on-cidoc-crm},
  keywords     = {virtual research environment, ontology, CIDOC CRM, ResearchSpace, VCMS},
}

@Misc{Alexiev2014-Brussels,
  author       = {Vladimir Alexiev},
  type         = {presentation},
  title        = {{Semantic Technologies for Cultural Heritage}},
  booktitle    = {SmartCulture Conference},
  year         = 2014,
  month        = jun,
  address      = {Brussels, Belgium},
  url_Slides   = {http://vladimiralexiev.github.io/pres/20140611-SmartCulture-sem-tech-CH},
  url_Other    = {http://vladimiralexiev.github.io/pres/20140611-SmartCulture-sem-tech-CH/Semantic Technologies for Cultural Heritage.pdf},
  keywords     = {semantic technology, ontology, semantic integration, cultural heritage},
}

@TechReport{Alexiev2014-ExtendingOWL2,
  author       = {Vladimir Alexiev},
  title        = {{Extending OWL2 Property Constructs with OWLIM Rules}},
  institution  = {Ontotext Corp},
  year         = 2014,
  number       = {(unpublished draft)},
  month        = sep,
  url          = {http://vladimiralexiev.github.io/pres/extending-owl2},
  keywords     = {ontology, OWL2, Property Chain Axiom, sub-property, property inferencing, transitive properies, Ontotext GraphDB},
  abstract     = {While OWL2 has very powerful class constructs, its property constructs are quite weak. We propose several extensions that we found useful, and implement them using OWLIM rules},
}

@Misc{Alexiev2014-GVP-LOD,
  author       = {Vladimir Alexiev},
  title        = {{Getty Vocabulary Program LOD: Ontologies and Semantic Representation}},
  booktitle    = {CIDOC Congress},
  year         = 2014,
  month        = sep,
  type         = {presentation},
  address      = {Dresden, Germany},
  url_Slides   = {http://vladimiralexiev.github.io/pres/20140905-CIDOC-GVP},
  url_Other    = {http://vladimiralexiev.github.io/pres/20140905-CIDOC-GVP/GVP-LOD-CIDOC.pdf},
  keywords     = {Getty, AAT, LOD, thesauri, vocabularies, SKOS, SKOS-XL, ISO 25964},
}

@Manual{Alexiev2014-GraphDBRuleProfiling,
  title        = {{Ontotext GraphDB Rules Optimisations}},
  author       = {Vladimir Alexiev},
  month        = dec,
  year         = 2014,
  url          = {http://graphdb.ontotext.com/documentation/standard/rules-optimisations.html},
  keywords     = {Ontotext GraphDB, inference, performance, optimization, profiling},
  abstract     = {GraphDB 6 includes a useful new feature that allows you to debug rule performance. We also include Optimization Hints for ruleset performance.},
}

@Misc{Alexiev2014-LinguisticLD,
  author       = {Vladimir Alexiev},
  title        = {{Linguistic Linked Data}},
  month        = oct,
  year         = 2014,
  url_Slides   = {http://vladimiralexiev.github.io/Multisensor/20141008-Linguistic-LD},
  keywords     = {Linguistic Linked Data, NLP, NLP2RDF, NIF, OLIA, NERD, MARL, BabelNet, FrameNet, WordNet},
  booktitle    = {Multisensor Project Meeting},
  type         = {presentation},
  address      = {Bonn, Germany},
  abstract     = {There's been a huge drive in recent years to represent NLP data as RDF. NLP data is usually large, so does it make sense to represent it as RDF? What's the benefit? Ontologies, schemas and groups include: GRaF ITS2 FISE LAF LD4LT LEMON LIME LMF MARL NERD NIF NLP2RDF OLIA OntoLex OntoLing OntoTag Penn Stanford... my oh my! There are a lot of linguistic resources available that can be used profitably: BabelNet FrameNet GOLD ISOcat LemonUBY Multitext OmegaNet UBY VerbNet Wiktionary WordNet.},
}

@Misc{Alexiev2014-Malmo,
  author       = {Vladimir Alexiev},
  title        = {{Semantic Technologies for Cultural Heritage}},
  month        = aug,
  year         = 2014,
  url_Slides   = {http://vladimiralexiev.github.io/pres/20140821-Malmo},
  url_Other    = {http://vladimiralexiev.github.io/pres/20140821-Malmo/SemTechCH-Malmo.pdf},
  keywords     = {semantic technology, ontology, semantic integration, cultural heritage},
  type         = {presentation},
  booktitle    = {Malmo Linked Data Meetup},
  address      = {Malmo, Sweden},
}

@InProceedings{Alexiev2014-NKOS,
  author       = {Vladimir Alexiev and Jutta Lindenthal and Antoine Isaac},
  title        = {{On Compositionality of ISO 25964 Hierarchical Relations (BTG, BTP, BTI)}},
  booktitle    = {13th European Networked Knowledge Organization Systems (NKOS 2014)},
  year         = 2014,
  month        = sep,
  address      = {London, UK},
  url_Slides   = {http://vladimiralexiev.github.io/pres/20140912-NKOS-compositionality},
  url_Other    = {http://vladimiralexiev.github.io/pres/20140912-NKOS-compositionality/BTG-BTP-BTI-compositionality.pdf},
}

@Misc{Alexiev2014-UWash,
  author       = {Vladimir Alexiev},
  title        = {{Doing Business with Semantic Technologies}},
  howpublished = {INFX 598 - Introducing Linked Data: concepts, methods and tools. Information School, University of Washington. Module 9. Guest lecture},
  month        = may,
  year         = 2014,
  url_Slides   = {http://www.slideshare.net/valexiev1/20140521-semtechbizguestlecture},
  url_Other    = {https://voicethread.com/myvoice/#thread/5784646/29625471/31274564},
  keywords     = {semantic technology},
  type         = {presentation},
  abstract     = {Introduction to Ontotext and some of its products, clients and projects},
}

@TechReport{Alexiev2015-CH-names,
  author       = {Vladimir Alexiev},
  title        = {{Name Data Sources for Semantic Enrichment}},
  institution  = {Europeana Creative project},
  year         = 2015,
  type         = {Deliverable},
  number       = {Part of Deliverable D2.4},
  month        = feb,
  url          = {http://vladimiralexiev.github.io/CH-names/README.html},
  keywords     = {Europeana, semantic enrichment, knowledge base, gazetteer, VIAF, Wikidata, ULAN, ISNI},
  abstract     = {Semantic enrichment in Europeana is a very difficult task due to several factors: 1. Varying metadata quality across different collections, sometimes including misallocation of metadata fields; 2. Varying metadata formatting practices across different collections, e.g. some collections indicate the role of a creator in brackets after the creator name; 3. Lack of accurate language information. In this report we focus on Person & Institution enrichment (person Named Entity Recognition), which in itself is an ambitious task. Historic people are often referred to by many names. For successful semantic enrichment it's important to integrate high-quality and high-coverage datasets that provide name info. There is a great number of Name Authority files maintained at libraries, museums and other heritage institutions world-wide, e.g. VIAF, ISNI, Getty ULAN, British Museum. Linked Open Data (LOD) datasets also have a plethora of names, e.g. in DBpedia, Wikidata and FreeBase. We analyze some of the available datasets in terms of person coverage, name coverage, language tags, extra features that can be useful for enrichment, quality. We also analyze the important topic of coreferencing, i.e. how connected the sources are to each other.},
}

@TechReport{Alexiev2015-EFD-classification,
  author       = {Vladimir Alexiev},
  title        = {{Europeana Food and Drink Classification Scheme}},
  institution  = {Europeana Food and Drink project},
  year         = 2015,
  type         = {Deliverable},
  number       = {D2.2},
  month        = feb,
  url          = {http://vladimiralexiev.github.io/pubs/Europeana-Food-and-Drink-Classification-Scheme-(D2.2).pdf},
  url_Slides   = {http://www.slideshare.net/valexiev1/europeana-food-and-drink-classification-scheme},
  keywords     = {Europeana, cultural heritage, food and drink, classification, categorization, DBpedia, Wikipedia, AGROVOC, WordNet, UMBEL},
  abstract     = {The Europeana Food and Drink Classification scheme (EFD classification) is a multi-dimensional scheme for discovering and classifying Cultural Heritage Objects (CHO) related to Food and Drink (FD). The topic of Food and Drink is so pervasive in our daily lives and in our culture that assembling a small "specialist" thesaurus is not feasible (such specialist thesauri were successfully used in other Europeana projects, eg ECLAP on performing arts and PartagePlus on Art Nouveau). We investigate about 20 datasets for their relevance to FD, including the Getty theasuri, Wordnet FD Domain, Wikipedia (in its 2 semantic data representations: DBpedia and Wikidata), AGROVOC, etc. We have selected Wikipedia as the basis for the classification, and plan to use the Wikipedia Categories to construct a hierarchical network to be used for classification. The project will also use innovative semantic technologies to automate the extraction of terms and co-references. The result will be a body of semantically-enriched metadata that can support a wider range of multi-lingual applications such as search, discovery and browsing. (91 pages)},
}

@TechReport{Alexiev2015-EFD-semapp-spec,
  author       = {Vladimir Alexiev},
  title        = {{Europeana Food and Drink Semantic Demonstrator Specification}},
  institution  = {Europeana Food and Drink project},
  year         = 2015,
  type         = {Deliverable},
  number       = {D3.19},
  month        = mar,
  url          = {http://vladimiralexiev.github.io/pubs/Europeana-Food-and-Drink-Semantic-Demonstrator-Specification-(D3.19).pdf},
  keywords     = {Europeana, cultural heritage, food and drink, semantic application, semantic search, faceted search, semantic enrichment},
  abstract     = {The Europeana Food and Drink Semantic Demonstrator (EFD sem app) will allow multi-dimensional semantic exploration and discovery of cultural heritage objects (CHO) related to Food and Drink (FD). It will both apply and augment the EFD Classification scheme, using positive feedback loop mechanisms: the more the classification is used, the better it becomes. It will enable providers to classify their content, and consumers to explore CHOs using semantic search},
}

@TechReport{Alexiev2015-EFD-semapp-progress1,
  author       = {Vladimir Alexiev},
  title        = {{Europeana Food and Drink Semantic Demonstrator M18 Progress Report}},
  institution  = {Europeana Food and Drink project},
  year         = 2015,
  type         = {Progress Report},
  number       = {D3.20a},
  month        = jun,
  url          = {http://vladimiralexiev.github.io/pubs/Europeana-Food-and-Drink-Semantic-Demonstrator-M18-Report-(D3.20a).pdf},
  keywords     = {Europeana, cultural heritage, food and drink, semantic application, semantic search, faceted search, semantic enrichment},
  abstract     = {Describes the development progress on the Europeana Food and Drink Semantic Demonstrator for the first 2.5 months (between 1 April 2015 and 15 June 2015), the achieved results, and project management considerations.},
}

@TechReport{AlexievTolosi2015-EFD-semapp-progress2,
  author       = {Vladimir Alexiev and Laura Tolosi},
  title        = {{Europeana Food and Drink Semantic Demonstrator M21 Progress Report}},
  institution  = {Europeana Food and Drink project},
  year         = 2015,
  type         = {Progress Report},
  number       = {D3.20b},
  month        = oct,
  url          = {http://vladimiralexiev.github.io/pubs/Europeana-Food-and-Drink-Semantic-Demonstrator-M21-Report-(D3.20b).pdf},
  keywords     = {Europeana, cultural heritage, food and drink, semantic application, semantic search, faceted search, semantic enrichment},
  abstract     = {This document describes the progress on developing the EFD Semantic Demonstrator for the 3 months from 1 Jul 2015 to 1 Oct 2015. We describe all work performed, the achieved results and project management considerations.},
}

@TechReport{Alexiev2015-EFD-semapp,
  author       = {Vladimir Alexiev},
  title        = {{Europeana Food and Drink Semantic Demonstrator Delivery}},
  institution  = {Europeana Food and Drink project},
  year         = 2015,
  type         = {Deliverable},
  number       = {D3.20},
  month        = oct,
  url          = {http://vladimiralexiev.github.io/pubs/Europeana-Food-and-Drink-Semantic-Demonstrator-Delivery-(D3.20).pdf},
  keywords     = {Europeana, cultural heritage, food and drink, semantic application, semantic search, faceted search, semantic enrichment},
  abstract     = {This document describes the development and delivery of the EFD Semantic Demonstrator. We describe all work performed between 1 April 2015 and 31 October 2015, the achieved results, the created data and enrichments, and the developed application.},
}

@Misc{Alexiev2015-GLAMs-Wikidata,
  author       = {Vladimir Alexiev},
  title        = {{GLAMs Working with Wikidata}},
  month        = may,
  year         = 2015,
  url_Slides   = {http://www.slideshare.net/valexiev1/glams-working-with-wikidata},
  keywords     = {Wikidata, Wikipedia, cultural heritage},
  type         = {presentation},
  booktitle    = {Europeana Food and Drink content provider workshop},
  address      = {Athens, Greece},
  abstract     = {How GLAMs can use Wikipedia/Wikidata to make their collections globally accessible across languages.},
}

@Article{Alexiev2015-IJDL,
  author       = {Vladimir Alexiev and Jutta Lindenthal and Antoine Isaac},
  title        = {{On the composition of ISO 25964 hierarchical relations (BTG, BTP, BTI)}},
  journal      = {International Journal on Digital Libraries},
  year         = 2015,
  pages        = {1-10},
  month        = aug,
  url          = {http://link.springer.com/content/pdf/10.1007/s00799-015-0162-2.pdf},
  url_Published= {http://link.springer.com/article/10.1007/s00799-015-0162-2},
  keywords     = {Thesauri, ISO 25964, BTG, BTP, BTI, Broader generic, Broader partitive, Broader instantial, AAT},
  issn         = {1432-1300},
  publisher    = {Springer},
  language     = {English},
  doi          = {10.1007/s00799-015-0162-2},
  abstract     = {Knowledge organization systems (KOS) can use different types of hierarchical relations: broader generic (BTG), broader partitive (BTP), and broader instantial (BTI). The latest ISO standard on thesauri (ISO 25964) has formalized these relations in a corresponding OWL ontology and expressed them as properties: broaderGeneric, broaderPartitive, and broaderInstantial, respectively. These relations are used in actual thesaurus data. The compositionality of these types of hierarchical relations has not been investigated systematically yet. They all contribute to the general broader (BT) thesaurus relation and its transitive generalization broader transitive defined in the SKOS model for representing KOS. But specialized relationship types cannot be arbitrarily combined to produce new statements that have the same semantic precision, leading to cases where inference of broader transitive relationships may be misleading. We define Extended properties (BTGE, BTPE, BTIE) and analyze which compositions of the original “one-step” properties and the Extended properties are appropriate. This enables providing the new properties with valuable semantics usable, e.g., for fine-grained information retrieval purposes. In addition, we relax some of the constraints assigned to the ISO properties, namely the fact that hierarchical relationships apply to SKOS concepts only. This allows us to apply them to the Getty Art and Architecture Thesaurus (AAT), where they are also used for non-concepts (facets, hierarchy names, guide terms). In this paper, we present extensive examples derived from the recent publication of AAT as linked open data.},
}

@Misc{Alexiev2015-bg.dbpedia,
  author       = {Vladimir Alexiev},
  title        = {{bg.dbpedia.org launched}},
  month        = feb,
  year         = 2015,
  url_Slides   = {http://vladimiralexiev.github.io/pres/20150209-dbpedia/bg-dbpedia-launched.html},
  url_Other    = {http://vladimiralexiev.github.io/pres/20150209-dbpedia/bg-dbpedia-launched.pdf},
  keywords     = {DBpedia},
  type         = {presentation},
  booktitle    = {DBpedia Meeting},
  address      = {Dublin, Ireland},
}

@Misc{Alexiev2015-dbpedia-mapping,
  author       = {Vladimir Alexiev},
  title        = {{Adding a DBpedia Mapping}},
  month        = feb,
  year         = 2015,
  url_Slides   = {http://vladimiralexiev.github.io/pres/20150209-dbpedia/add-mapping.html},
  url_Other    = {http://vladimiralexiev.github.io/pres/20150209-dbpedia/add-mapping-long.html},
  keywords     = {DBpedia, ontology mapping},
  type         = {presentation},
  booktitle    = {DBpedia Meeting},
  address      = {Dublin, Ireland},
}

@Misc{Alexiev2015-dbpedia-problems,
  author       = {Vladimir Alexiev},
  title        = {{DBpedia Ontology and Mapping Problems}},
  month        = feb,
  year         = 2015,
  url_Slides   = {http://vladimiralexiev.github.io/pres/20150209-dbpedia/dbpedia-problems.html},
  url_Other    = {http://vladimiralexiev.github.io/pres/20150209-dbpedia/dbpedia-problems-long.html},
  keywords     = {DBpedia, ontology, ontology mapping, data quality},
  type         = {presentation},
  booktitle    = {DBpedia Meeting},
  address      = {Dublin, Ireland},
}

@Misc{AlexievAngelova-CultJam15,
  author       = {Vladimir Alexiev and Dilyana Angelova},
  title        = {{O is for Open: OAI and SPARQL interfaces for Europeana}},
  month        = jul,
  year         = 2015,
  url_Slides   = {http://vladimiralexiev.github.io/pubs/O_is_for_Open_(CultJam_201507)_slide.pdf},
  url_Other    = {http://vladimiralexiev.github.io/pubs/O_is_for_Open_(CultJam_201507)_poster.pdf},
  keywords     = {Europeana, OAI, OAI PMH, SPARQL, EDM, semantic repository},
  type         = {poster},
  booktitle    = {Europeana Creative Culture Jam},
  address      = {Vienna, Austria},
  abstract     = {Poster. As part of the Europeana Creative project, Ontotext added 2 additional channels to Europeana Labs: OAI & SPARQL, complementing the API. OAI is used for bulk donwload (e.g. to update the semantic repository). SPARQL can answer queries that the API cannot, e.g. linking objects, exploring contextual entities (e.g. parent places or author life dates), analytics/charts},
}

@InProceedings{AlexievAsenova2010-TeachingIT_PM,
  author       = {Vladimir Alexiev and Petya Asenova},
  title        = {{An Approach to Teaching IT Project Management in a Masters Program}},
  booktitle    = {6th Annual International Conference on Education in Computer Science},
  year         = 2010,
  month        = jun,
  address      = {Fulda and Munich, Germany},
  url          = {http://vladimiralexiev.github.io/pubs/AlexievAsenova2010-TeachingIT_PM.pdf},
  keywords     = {Masters Program, NBU, IT project management, PM, university curriculum},
  abstract     = {Many Bulgarian IT professionals manage projects but their knowledge and skills in this area are based mainly on their own experience, which is often obtained through trial and error. Although the project manager (PM) has a crucial role for project success, the university curriculum in Bulgaria does not answer sufficiently these business needs. Some aspects of PM are included in university courses on Software Engineering and some short courses on IT PM are offered, but as overall this matter is not covered in depth in any national university. Having in mind this real need, we proposed a new Masters Program on IT PM hoping it will meet the interest of many students representatives of the software business. This paper presents an approach to prepare PMs for the Bulgarian IT industry through a Masters Program, developed in cooperation between the New Bulgarian University (NBU) and the Institute of Mathematics and Informatics (IMI) of the Bulgarian Academy of Science (BAS). We describe the background, objectives and design of the program, and relations with the business.},
}

@Book{AlexievBreuBruijn2005-InformationIntegration,
  author       = {Vladimir Alexiev and Michael Breu and Jos de Bruijn and Dieter Fensel and Ruben Lara and Holger Lausen},
  title        = {{Information Integration with Ontologies: Experiences from an Industrial Showcase}},
  publisher    = {John Wiley and Sons},
  year         = 2005,
  month        = feb,
  url_Published= {http://eu.wiley.com/WileyCDA/WileyTitle/productCd-0470010487.html},
  keywords     = {semantic integration, ontology, semantic technology, ontology-based data access},
  chapter      = 2,
  isbn         = {978-0-470-01048-8},
  abstract     = {Disparate information, spread over various sources, in various formats, and with inconsistent semantics is a major obstacle for enterprises to use this information at its full potential. Information Grids should allow for the effective access, extraction and linking of dispersed information. Currently Europe's coporations spend over 10 Billion EUR to deal with these problems. This book will demonstrate the applicability of grid technologies to industry. To this end, it gives a detailed insight on how on tology technology can be used to manage dispersed information assets more efficiently. The book is based on experiences from the COG (Corporate Ontology Grid) project, carried out jointly by three leading industrial players and the Digital Enterprise Research Institute Austria. Through comparisons of this project with alternative technologies and projects, it provides hands-on experience and best practice examples to act as a reference guide for their development. Information Integration with Ontologies: Ontology based Information Integration in an Industrial Setting is ideal for technical experts and computer researchers in the IT-area looking to achieve integration of heterogeneous information and apply ontology technologies and techniques in practice. It will also be of great benefit to technical decision makers seeking infor mation about ontology technologies and the scientific audience, interested in achievements towards the application of ontologies in an industrial setting.},
}

@InProceedings{AlexievCasamayor-FN-NIF,
  author       = {Vladimir Alexiev and Gerard Casamayor},
  title        = {{FN goes NIF: Integrating FrameNet in the NLP Interchange Format}},
  booktitle    = {Linked Data in Linguistics (LDL-2016): Managing, Building and Using Linked Language Resources (accepted)},
  year         = 2016,
  month        = may,
  address      = {Portorož, Slovenia},
  url          = {http://vladimiralexiev.github.io/Multisensor/FrameNet/paper.pdf},
  keywords     = {linguistic linked data, FrameNet, NIF, NLP2RDF, RDF, application profile},
  abstract     = {FrameNet (FN) is a large-scale lexical database for English developed at ICSI Berkeley that describes word senses in terms of Frame semantics. FN has been converted to RDF LOD by ISTC-CNR, together with a large corpus of text annotated with FN. NIF is an RDF/OWL format and protocol for exchanging text annotations between NLP tools as Linguistic Linked Data. This paper reviews the FN-LOD representation, compares it to NIF, and describes a simple way to integrate FN in NIF, which does not use any custom classes or properties.},
}

@Article{AlexievMartev2009-ElectronicExportBG,
  author       = {Vladimir Alexiev and Teodor Martev},
  title        = {{Electronic Export Declarations Ease the Work of the Customs Agency and Traders}},
  journal      = {Computerworld (in Bulgarian)},
  year         = 2009,
  volume       = 46,
  month        = dec,
  annote       = {ECS2 was nominated for IT Project of the year 2010},
  url_Slides   = {http://vladimiralexiev.github.io/pubs/AlexievMartev2009-ElectronicExportBG.pdf},
  keywords     = {model-driven development, e-customs, e-government, Export Control System},
}

@Misc{AlexievMitevBukev2010-eGovBPM,
  author       = {Vladimir Alexiev and Adrian Mitev and Alexander Bukev},
  title        = {{Implementing complex e-Government solutions with open source and BPM: Architecture of Export Control System phase 2 (ECS2)}},
  year         = 2010,
  url_Slides   = {http://vladimiralexiev.github.io/pubs/AlexievMitevBukev2010-eGovBPM.pdf},
  keywords     = {model-driven development, business process management, BPMS, e-customs, e-government, Export Control System, system architecture},
  type         = {presentation},
  booktitle    = {Java2Days Conference},
  address      = {Sofia, Bulgaria},
}

@TechReport{EDM-FRBRoo,
  author       = {Martin Doerr and Stefan Gradmann and others},
  title        = {{Europeana Task Force on EDM-FRBRoo Application Profile}},
  institution  = {Europeana},
  year         = 2013,
  month        = may,
  url          = {http://pro.europeana.eu/get-involved/europeana-tech/europeanatech-task-forces/edm-frbroo-application-profile},
  abstract     = {The EDM – FRBRoo Application Profile Task Force (EFAP-TF) was launched in response to the recommendations from the deliverable D3.4 from Europeana V1.0. This deliverable asked for an application profile that would allow a better representation of the FRBR group 1 entities: work, expression, manifestation and item. Additionally, it was to be conceived as an application profile of FRBRoo where each intellectual contribution (e.g., in the publication process) and the related activity are treated as entities in their own right, and does not depend too much on the notion of a bibliographic record. As a starting point they suggested the mapping of FRBRoo and EDM offered by the CIDOC CRM working group. The aim of the EFAP-TF is to extend, correct or restrict this suggested mapping and provide examples for the use of the combined EDM and FRBRoo namespaces.
This report delivers combined models in terms of properties and classes of EDM and FRBRoo illustrated by sample data. Smaller groups have worked on three different examples. The report also provides principles for modeling and mapping rules based on the experiments of the working groups.},
keywords={Europeana, task force, EDM, FRBR, FRBRoo, CIDOC CRM, ontology}
}

@TechReport{Europeana-enrichment-strategy,
  author       = {Juliane Stiller and Antoine Isaac and Vivien Petras and others},
  title        = {{EuropeanaTech Task Force on a Multilingual and Semantic Enrichment Strategy}},
  institution  = {Europeana},
  year         = 2014,
  month        = apr,
  url          = {http://pro.europeana.eu/get-involved/europeana-tech/europeanatech-task-forces/multilingual-and-semantic-enrichment-strategy},
  keywords     = {cultural heritage, Europeana, task force, semantic enrichment, text analysis, multilingual, evaluation},
  abstract     = {The semantic and multilingual enrichment of metadata in Europeana is a core concern as it improves access to the material, defines relations among objects and enables cross-lingual retrieval of documents. The quality of these enrichments is crucial to ensure that highly curated content from providers gets represented correctly across different languages. To ensure that those enrichments unfold their whole potential and act as facilitators of access, a semantic and multilingual enrichment strategy is needed. The EuropeanaTech Task Force on a Multilingual and Semantic Enrichment Strategy set out to analyze datasets in Europeana and to evaluate them with regard to their enrichment potential and the enrichments that were executed. The goal was to drive a strategy for enriching metadata fields adding value for users. To achieve this, the members of the task force held a one-day workshop in Berlin where they analyzed randomly selected datasets from Europeana, their metadata fields and their enrichment potential. This report aggregates the results and derives findings and recommendations regarding the metadata quality (source), vocabulary used (target) and the enrichment process. It was found that especially during mapping and ingestion time, metadata quality issues arise that influence the success of the enrichments. Tackling these issues with better documentation, training and the establishment of quality scores are some of the recommendations in this field. Furthermore, Europeana should encourage the delivery of specialized vocabularies with resolvable URIs which would also lead to less need for enrichments by Europeana itself. With regard to the enrichment process, clear rules for each field need to be established.},
}

@TechReport{Europeana-evaluation-enrichments,
  author       = {Antoine Isaac and Hugo Manguinhas and Juliane Stiller and Valentine Charles and others},
  title        = {{Report on Enrichment and Evaluation}},
  institution  = {Europeana Task Force on Enrichment and Evaluation},
  year         = 2015,
  month        = oct,
  url          = {http://pro.europeana.eu/taskforce/evaluation-and-enrichments},
  keywords     = {cultural heritage, Europeana, task force, semantic enrichment, text analysis, multilingual, evaluation},
  abstract     = {This report on Evaluation and Enrichment provides an overview of the different processes in semantic enrichment and offers guidance on how to assess each of these steps to implement a coherent enrichment strategy. The report begins by introducing the terminology used in the report. While defining the notion of semantic enrichment, the Task Force has identified several other associated notions that are commonly used in the cultural heritage domain when addressing semantic enrichment. We also provide an overview of the enrichment tools and services developed in the Europeana Network over the past years, reflecting the diversity of processes at hand: tools for manual enrichment and annotation, tools for automatic enrichment and workflow design tools. We also focus on the interoperability issues such as rules for specifying the linking or the format used to describe the enrichment outputs. As well as looking at the details of the enrichment processes we pick up the work done by the previous Task Force by specifying criteria for selecting and assessing target datasets. These criteria are based on vocabularies and datasets examples relevant to the Cultural Heritage domain. This selection strategy is available in a companion document to this report. The last component of the enrichment strategy is the evaluation of the enrichment processes. So far, evaluation in this domain has not been much documented even though a lot of work has been done in the field. We have tried to summarise different evaluation methodologies developed in related projects. These methods highlight the different components of the enrichment process that can be subject to evaluation. In order to validate all the recommendations provided in the previous sections, we have performed a quantitative and qualitative evaluation of seven enrichment services on a same subset of the Europeana dataset. The report of the evaluation is available in a companion document to this report while the main conclusions remain in this report. This report is a result of an inventory of tools, practices and standards that define the current state of the art for semantic enrichment. The analysis and evaluation work done during the course of the Task Force have allowed us to compile a series of lessons learnt that should be considered for the design and enhancement of enrichment services and their evaluation},
}

@Manual{GVP-LOD-doc,
  title        = {{Getty Vocabularies Linked Open Data: Semantic Representation}},
  author       = {Vladimir Alexiev and Joan Cobb and Gregg Garcia and Patricia Harpring},
  organization = {Getty Research Institute},
  edition      = {3.2},
  month        = mar,
  year         = 2015,
  url          = {http://vocab.getty.edu/doc/},
  keywords     = {Getty, vocabularies, thesauri, AAT, TGN, ULAN, semantic representation, LOD, ontology, SKOS, SKOS-XL, ISO 25964},
}

@Manual{GVP-LOD-queries,
  title        = {{Getty Vocabularies: LOD Sample Queries}},
  author       = {Vladimir Alexiev},
  organization = {Getty Research Institute},
  edition      = {3.2},
  month        = mar,
  year         = 2015,
  url          = {http://vocab.getty.edu/doc/queries/},
  abstract     = {We provide 120 sample queries for the Getty Vocabularies LOD that should allow you to learn to query the data effectively. We include searching for data, getting all data of a subject, all labels and their attributes, full-text search, getting an ordered hierarchy, charts, etc. The queries are organized in sections: general, TGN-specific, ULAN-specific, Language queries, Counting and descriptive info, Exploring the ontology},
  keywords     = {Getty, vocabularies, thesauri, AAT, TGN, ULAN, SPARQL, ontology, SKOS, SKOS-XL, ISO 25964},
}

@Manual{GVP-ontology,
  title        = {{Getty Vocabulary Program (GVP) Ontology}},
  author       = {Vladimir Alexiev},
  edition      = {3.2},
  month        = mar,
  year         = 2015,
  url          = {http://vocab.getty.edu/ontology},
  url_Other    = {http://lov.okfn.org/dataset/lov/details/vocabulary_gvp.html},
  keywords     = {Getty, vocabularies, thesauri, AAT, TGN, ULAN, semantic representation, LOD, ontology, SKOS, SKOS-XL, ISO 25964, DC, DCT, BIBO, FOAF, BIO, Schema, PROV, WGS84},
  institution  = {Getty Research Institute},
  type         = {Ontology},
  note         = {Ontology},
  abstract     = {The GVP Ontology defines classes, properties and values (skos:Concepts) used in GVP LOD. As of version 3.0, it is complete regarding AAT, TGN and ULAN, and will be extended in time with more elements needed for other GVP vocabularies (CONA). It uses the SKOS, SKOS-XL, ISO 25964; DC, DCT, BIBO, FOAF, BIO, Schema, PROV, WGS84 ontologies.},
}

@Misc{Alexiev2015-Glam-Wiki,
  author       = {Vladimir Alexiev and Valentine Charles and Hugo Manguinhas},
  title        = {{Wikidata, a Target for Europeana's Semantic Strategy}},
  month        = apr,
  year         = 2015,
  url_Slides   = {http://www.slideshare.net/valexiev1/wikidata-a-target-for-europeanas-semantic-strategy-glamwiki-2015},
  url_Other    = {http://vladimiralexiev.github.io/pubs/GLAMwiki2015.ppt},
  keywords     = {cultural heritage, GLAM, Europeana, semantic enrichment, Wikidata, Wikipedia},
  type         = {presentation},
  booktitle    = {Glam-Wiki 2015},
  address      = {The Hague},
  url_Other    = {https://nl.wikimedia.org/wiki/GLAM-WIKI_2015/Programme/Discussions/Strategy#Presentation:_Wikidata.2C_a_target_for_Europeana.E2.80.99s_semantic_strategy.3F},
  abstract     = {For Europeana, the platform for Europe’s digital cultural heritage from libraries, museums and archives, getting richer (semantic and multilingual) metadata is a priority. It improves access to the 40 million cultural heritage objects, notably enabling the multilingual retrieval of documents and creates relations between objects. To enhance data and enable retrieval across languages, Europeana performs automatic enrichment by selecting source metadata field(s) in the Europeana data and creating links to a selected target vocabulary or dataset representing contextual resources such as places, concepts, agents and time periods. Wikidata is since a while on Europeana’s radar as a potential new target for enrichment but how can it be integrated with cultural heritage data?},
}

@Article{HCLS-paper,
  author       = {Michel Dumontier and Alasdair J. G. Gray and M. Scott Marshall and Vladimir Alexiev and others},
  title        = {{The Health Care and Life Sciences Community Profile for Dataset Descriptions}},
  journal      = {(submitted)},
  year         = 2015,
  month        = nov,
  keywords     = {HCLS, dataset, VOID, ontology},
  abstract     = {Access to consistent, high-quality metadata is critical to finding, understanding, and reusing scientific data. However, while there are many relevant vocabularies for the annotation of a dataset, none sufficiently captures all the necessary metadata. This prevents uniform indexing and querying of dataset repositories. Towards providing a practical guide for producing a high quality description of biomedical datasets, the W3C Semantic Web for Health Care and the Life Sciences Interest Group (HCLSIG) identified RDF vocabularies that could be used to specify common metadata elements and their value sets. The resulting guideline covers elements of description, identification, attribution, versioning, provenance, and content summarization. This guideline reuses existing vocabularies, and is intended to meet key functional requirements including indexing, discovery, exchange, query, and retrieval of datasets. The resulting metadata profile is generic and could be used by other domains with an interest in providing machine readable descriptions of versioned datasets.},
}

@TechReport{HCLS-profile,
  title        = {{Dataset Descriptions: HCLS Community Profile}},
  author       = {Alasdair J. G. Gray and Joachim Baran abd M. Scott Marshall and Michel Dumontier and Vladimir Alexiev and others},
  month        = may,
  year         = 2015,
  url          = {http://www.w3.org/TR/hcls-dataset/},
  keywords     = {HCLS, dataset, VOID, ontology},
  institution  = {Semantic Web in Health Care and Life Sciences Interest Group (HCLSIG)},
  abstract     = {Access to consistent, high-quality metadata is critical to finding, understanding, and reusing scientific data. This document describes a consensus among participating stakeholders in the Health Care and the Life Sciences domain on the description of datasets using the Resource Description Framework (RDF). This specification meets key functional requirements, reuses existing vocabularies to the extent that it is possible, and addresses elements of data description, versioning, provenance, discovery, exchange, query, and retrieval.},
}

@Manual{ISO-25964-owl,
  title        = {{ISO 25964 Part 1: Thesauri for information retrieval: RDF/OWL vocabulary, extension of SKOS and SKOS-XL}},
  author       = {Johan De Smedt and Antoine Isaac and Stella Dextre Clarke and Jutta Lindenthal and Marcia Lei Zeng and Douglas S. Tudhope and Leonard Will and Vladimir Alexiev},
  month        = dec,
  year         = 2013,
  url          = {http://purl.org/iso25964/skos-thes},
  url_Other    = {http://lov.okfn.org/dataset/lov/details/vocabulary_iso-thes.html},
  type         = {Ontology},
  note         = {Ontology},
  abstract     = {OWL ontology representing the newest ISO standard on thesauri},
}

@InProceedings{Ikonomov2013-EuropeanaCreative-EDM,
  author       = {Nikola Ikonomov and Boyan Simeonov and Jana Parvanova and Vladimir Alexiev},
  title        = {{Europeana Creative. EDM Endpoint. Custom Views}},
  booktitle    = {Digital Presentation and Preservation of Cultural and Scientific Heritage (DiPP 2013)},
  year         = 2013,
  month        = sep,
  address      = {Veliko Tarnovo, Bulgaria},
  url          = {http://vladimiralexiev.github.io/pubs/Ikonomov2013-EuropeanaCreative-EDM.pdf},
  url_Slides   = {http://vladimiralexiev.github.io/pubs/Ikonomov2013-EuropeanaCreative-EDM-slides.pdf},
  keywords     = {cultural heritage, Europeana, EDM, ESE, semantic technology, RDF, SKOS, URI, Ontotext GraphDB, semantic repository, SPARQL, endpoint},
  abstract     = {The paper discusses the Europeana Creative project which aims to facilitate re-use of cultural heritage metadata and content by the creative industries. The paper focuses on the contribution of Ontotext to the project activities. The Europeana Data Model (EDM) is further discussed as a new proposal for structuring the data that Europeana will ingest, manage and publish. The advan-tages of using EDM instead of the current ESE metadata set are highlighted. Finally, Ontotext's EDM Endpoint is presented, based on OWLIM semantic re-pository and SPARQL query language. A user-friendly RDF view is presented in order to illustrate the possibilities of Forest - an extensible modular user interface framework for creating linked data and semantic web applications.},
}

@TechReport{LDBC-D442,
  author       = {Vassilis Papakonstantinou and Irini Fundulaki and Giorgos Flouris and Vladimir Alexiev},
  title        = {{Benchmark Design for Reasoning}},
  institution  = {Linked Data Benchmarking Council project},
  year         = 2014,
  type         = {Deliverable},
  number       = {D4.4.2},
  month        = sep,
  url          = {http://ldbcouncil.org/sites/default/files/LDBC_D4.4.2.pdf},
  keywords     = {LDBC, benchmark, reasoning, inference},
  abstract     = {Reasoning (mainly OWL reasoning) has received increasing attention by ontology designers for more accurately representing the domain at hand. To reflect this importance, one of LDBC’s objectives is to identify a set of interesting use cases that consider OWL reasoning constructs (beyond the usual RDFS constructs) that can be used to challenge existing RDF engines or repositories. This Deliverable has two parts: in the first part, we present four different sets of queries that can be used to determine whether RDF query engines take into account OWL constructs during query plan construction or query execution; in the second part we consider how a repository or query engine incorporates and considers business rules, i.e., domain-specific rules that follow common templates, useful in practical applications.},
}

@InProceedings{MarinovAlexievDjonev1994-BCPN,
  author       = {Georgi Marinov and Vladimir Alexiev and Yavor Djonev},
  title        = {{Boolean Constraint Propagation Networks}},
  booktitle    = {Artifical Intelligence: Methodology, Systems, and Applications (AIMSA'94)},
  year         = 1994,
  editor       = {P. Jorrand and V. Sgurev},
  pages        = {109-118},
  month        = sep,
  address      = {Sofia, Bulgaria},
  publisher    = {World Scientific Publishing},
  url          = {http://vladimiralexiev.github.io/pubs/MarinovAlexievDjonev1994-BCPN.pdf},
  url_Published= {http://dl.acm.org/citation.cfm?id=212113},
  keywords     = {constraint propagation, inference, knowledge-based system, expert system},
  isbn         = {981-02-1853-2},
  abstract     = {This paper describes a particular inference mechanism which has been successfully used for the implementation of an expert system and a generic shell supporting consulting-type expert systems. The main features of Boolean Constraint Propagation Networks (BCPN) are: the inference fows in all directions, unlike inference modes of forward or backward chaining systems; all possible consequences of a fact are derived as soon as the user enters the fact, therefore the system is very interactive; if the user withdraws an assertion then all propositions depending on it are retracted; the inference architecture is simple and uniform. After a general description of BCPN we give an account of the problems encountered and the approaches we used to solve them. Some possible extensions of the mechanism and its applicability to various areas are also discussed. The current version of BCPN is written in C++ and took about one man-year to develop.},
}

@TechReport{OldmanMahmudAlexiev2013,
  author       = {Dominic Oldman and Joshan Mahmud and Vladimir Alexiev},
  title        = {{The Conceptual Reference Model Revealed. Quality contextual data for research and engagement: A British Museum case study}},
  institution  = {ResearchSpace Project},
  year         = 2013,
  note         = {Draft 0.98},
  month        = jul,
  url          = {http://confluence.ontotext.com/display/ResearchSpace/BM+Mapping},
  keywords     = {cultural heritage, museum informatics, ontology, CIDOC CRM, semantic mapping, British Museum, ResearchSpace},
  pages        = {359 pages},
  abstract     = {Contents: 169p: Main body, including discussion, illustrations and mapping diagrams. 7p: Association Codes (see details at BM Association Mapping v2). 49p: Example Object Graph. 134p: RDFer configuration files (i.e. mapping implementation)},
}

@InProceedings{Parvanova2013-SemanticAnnotation,
  author       = {Jana Parvanova and Vladimir Alexiev and Stanislav Kostadinov},
  title        = {{RDF Data and Image Annotations in ResearchSpace}},
  booktitle    = {{International Workshop on Collaborative Annotations in Shared Environment: metadata, vocabularies and techniques in the Digital Humanities (DH-CASE 2013). Collocated with DocEng 2013}},
  year         = 2013,
  month        = sep,
  address      = {Florence, Italy},
  url          = {http://vladimiralexiev.github.io/pubs/Parvanova2013-SemanticAnnotation.pdf},
  url_Slides   = {http://vladimiralexiev.github.io/pubs/Parvanova2013-SemanticAnnotation-slides.pdf},
  url_Published= {http://dl.acm.org/citation.cfm?id=2517997},
  keywords     = {Computer-supported collaborative work, Annotation, Museum informatics, Cultural heritage, ResearchSpace, SVG, Web Annotation, onology, British Museum, CIDOC CRM},
  abstract     = {This paper presents the approaches to data and image annotation in ResearchSpace (http://www.researchspace.org), an Andrew W. Mellon Foundation funded project led by the British Museum aimed at supporting collaborative internet research, information sharing and web applications for the cultural heritage scholarly community},
  isbn         = {978-1-4503-2199-0},
  doi          = {10.1145/2517978.2517997},
}

@InProceedings{Multisensor-ICMEW2015,
  author       = {Stefanos Vrochidis and Ioannis Kompatsiaris and Gerard Casamayor and Ioannis Arapakis and Reinhard Busch and Vladimir Alexiev and Emmanuel Jamin and Michael Jugov and Nicolaus Heise and Teresa Forrellat and Dimitris Liparas and Leo Wanner and Iris Miliaraki and Vera Aleksic and Kiril Simov and Alan Mas Soro and Mirja Eckhoff and Tilman Wagner and Marti Puigbo},
  title        = {{MULTISENSOR: Development of Multimedia Content Integration Technologies for Journalism, Media Monitoring and International Exporting Decision Support}},
  booktitle    = {{2015 IEEE International Conference on Multimedia & Expo Workshops (ICMEW)}},
  year         = 2015,
  pages        = {1-4},
  month        = jun,
  address      = {Turin, Italy},
  url_Published= {http://www.computer.org/csdl/proceedings/icmew/2015/7079/00/07169818.pdf},
  keywords     = {Multisensor, semantic enrichment, NLP, multimedia annotation},
  doi          = {10.1109/ICMEW.2015.7169818},
  abstract     = {This paper presents an overview and the first results of the FP7 MULTISENSOR project, which deals with multidimensional content integration of multimedia content for intelligent sentiment enriched and context oriented interpretation. MULTISENSOR aims at providing unified access to multilingual, multimedia and multicultural economic, news story material across borders in order to support journalism and media monitoring tasks and provide decision support for internationalisation of companies.},
}

@InProceedings{TagarevTolosiAlexiev-FD,
  author       = {Andrey Tagarev and Laura Tolosi and Vladimir Alexiev},
  title        = {{Domain-specific modeling: Towards a Food and Drink Gazetteer}},
  booktitle    = {First International Keystone Conference},
  year         = 2015,
  month        = sep,
  address      = {Coimbra, Portugal},
  url          = {http://vladimiralexiev.github.io/pubs/Tagarev2015-DomainSpecificGazetteer.pdf},
  url_Slides   = {http://vladimiralexiev.github.io/pubs/Tagarev2015-DomainSpecificGazetteer-slides.pdf},
  keywords     = {classification, categorization, Wikipedia, DBpedia, gazetteer, Europeana, Cultural Heritage, concept extraction, semantic enrichment, food and drink},
  abstract     = {Our goal is to build a Food and Drink (FD) gazetteer that can serve for classification of general, FD-related concepts, efficient faceted search or automated semantic enrichment. Fully supervised design of a domain-specific models "ex novo" is not scalable. Integration of several ready knowledge bases is tedious and does not ensure coverage. Completely data-driven approaches require a large amount of training data, which is not always available. In cases when the domain is not very specific (as the FD domain), re-using encyclopedic knowledge bases like Wikipedia may be a good idea. We propose here a semi-supervised approach, that uses a restricted Wikipedia as a base for the modeling, achieved by selecting a domain-relevant Wikipedia category as root for the model and all its subcategories, combined with expert and data-driven pruning of irrelevant categories.},
}

@InProceedings{TagarevTolosiAlexiev-FD-extended,
  author       = {Andrey Tagarev and Laura Tolosi and Vladimir Alexiev},
  title        = {{Domain-specific modeling: Towards a Food and Drink Gazetteer (extended version)}},
  booktitle    = {{Semantic Keyword-based Search on Structured Data Sources}},
  year         = 2016,
  editor       = {Jorge Cardoso and Francesco Guerra and Geert-Jan Houben and Alexandre Miguel Pinto and Yannis Velegrakis},
  publisher    = {Springer},
  url_Published= {http://link.springer.com/chapter/10.1007/978-3-319-27932-9_16},
  url_Preprint = {http://vladimiralexiev.github.io/pubs/Tagarev2015-DomainSpecificGazetteer-extended.pdf},
  chapter      = 16,
  doi          = {10.1007/978-3-319-27932-9_16},
  isbn         = {978-3-319-27932-9},
  volume       = 9398,
  series       = {Lecture Notes in Computer Science},
  pages        = {182-196},
  month        = jan,
  keywords     = {classification, categorization, Wikipedia, DBpedia, gazetteer, Europeana, Cultural Heritage, concept extraction, semantic enrichment, food and drink},
  abstract     = {Our goal is to build a Food and Drink (FD) gazetteer that can serve for classification of general, FD-related concepts, efficient faceted search or automated semantic enrichment. Fully supervised design of a domain-specific models ex novo is not scalable. Integration of several ready knowledge bases is tedious and does not ensure coverage. Completely data-driven approaches require a large amount of training data, which is not always available. For general domains (such as the FD domain), re-using encyclopedic knowledge bases like Wikipedia may be a good idea. We propose here a semi-supervised approach that uses a restricted Wikipedia as a base for the modeling, achieved by selecting a domain-relevant Wikipedia category as root for the model and all its subcategories, combined with expert and data-driven pruning of irrelevant categories.},
}

@Misc{Zeng-NKOS2015,
  author       = {Marcia Zeng and Julaine Clunis and Vladimir Alexiev},
  title        = {{Innovative Use of KOS that are Published as Linked Open Data (LOD)}},
  month        = dec,
  year         = 2015,
  url_Slides   = {http://nkos.slis.kent.edu/2015NKOSworkshop/MarciaZeng-LODKOSInnovativeUse.pdf},
  url_Other    = {http://vladimiralexiev.github.io/pubs/MarciaZeng-LODKOSInnovativeUse.pdf},
  keywords     = {vocabularies, thesauri, SKOS, NKOS, ontology},
  type         = {presentation},
  booktitle    = {First NKOS Workshop at International Conference on Asian Digital Libraries (ICADL 2015)},
  address      = {Yonsei University, Seoul, Korea},
}

@Misc{Alexiev-EFD-DBpedia,
  author       = {Vladimir Alexiev},
  title        = {{Using DBPedia in Europeana Food and Drink}},
  month        = feb,
  year         = 2016,
  url_Slides   = {http://vladimiralexiev.github.io/pres/20160212-Using-DBPedia-in-Europeana-Food-and-Drink.pdf},
  keywords     = {Europeana, cultural heritage, food and drink, DBpedia, Geonames, semantic application, faceted search, semantic search},
  booktitle    = {DBpedia Meeting},
  type         = {presentation},
  address      = {The Hague, Netherlands},
  url_Slides   = {https://drive.google.com/file/d/0B7je1jgVmCgIZzNiWmdqTGpDa28/view},
  abstract     = {The Europeana Food and Drink project collects cultural heritage objects for and develops applications related to Food and Drink heritage. As part of the project, Ontotext developed a FD Classification based on Wikipedia/DBpedia Categories, a semantic enrichment service that annotates each CHO with FD Topics and Places, and a semantic application (http://efd.ontotext.com/app) that implements hierarchical semantic facets and semantic search for these facets. We'll also be packaging the enrichment as a service for others to use in a crowdsourced annotation application. We will explain how we used Categories to build a domain-specific gazetteer, used external datasets (eg UMBEL domains and DBTax types), correlated DBpedia places to Geonames to use the place hierarchy, and the workings of the semantic application},
}

@Misc{Alexiev-EuropeanaMC-blog,
  author       = {Vladimir Alexiev},
  title        = {{Meet the Europeana Members Council: Vladimir Alexiev}},
  month        = mar,
  year         = 2016,
  keywords     = {cultural heritage, Europeana, EHRI, ResearchSpace, data quality, semantic enrichment},
  type         = {blog},
  url_blog     = {http://pro.europeana.eu/blogpost/meet-the-members-council-vladimir-alexiev},
  abstract     = {Describes the work of Ontotext and in particular Vladimir Alexiev in applying semantic technologies to cultural heritage},
}

@Misc{Alexiev-rdfpuml,
  author       = {Vladimir Alexiev},
  title        = {{Making True RDF Diagrams with rdfpuml}},
  month        = mar,
  year         = 2016,
  url_Slides   = {http://vladimiralexiev.github.io/pres/20160325-rdfpuml},
  keywords     = {RDF, visualization, PlantUML, cultural heritage, NLP, NIF, EHRI},
  type         = {presentation},
  abstract     = {RDF is a graph data model, thus often the best way to understand RDF data schemas (ontologies, application profiles, RDF shapes) is with a diagram. We describe a tool (rdfpuml) that makes true diagrams from Turtle examples using PlantUML and GraphViz. Diagram readability is of prime concern, and rdfpuml introduces a few diagram control mechanisms using triples in the puml: namespace. We give examples from Getty CONA (Mappings of museum data to CIDOC CRM), Multisensor (NLP2RDF/NIF, FrameNet), EHRI (Holocaust Research into Jewish social networks), Duraspace (Portland Common Data Model for holding metadata in institutional repositories)},
}

@Misc{Alexiev-OpenData2016,
  author       = {Vladimir Alexiev},
  title        = {{How to find Open Data and Ontologies in Linguistics/NLP and Cultural Heritage}},
  month        = mar,
  year         = 2016,
  url_Slides   = {http://vladimiralexiev.github.io/pres/20160329-OpenData-and-Ontologies},
  type         = {presentation},
  booktitle    = {4th Open Data & Linked Data meetup},
  address      = {Sofia, Bulgaria},
}

